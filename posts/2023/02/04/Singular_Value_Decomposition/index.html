<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <title>Tech with Deb</title>
    <meta content="" name="description" />
    <meta content="" name="keywords" />

    <!-- Favicons -->
    <link href="../../../../../assets/img/favicon.png" rel="icon" />
    <link
      href="../../../../../assets/img/apple-touch-icon.png"
      rel="apple-touch-icon"
    />

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Raleway:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap"
      rel="stylesheet"
    />

    <!-- Vendor CSS Files -->
    <link
      href="../../../../../assets/vendor/bootstrap/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <link
      href="../../../../../assets/vendor/bootstrap-icons/bootstrap-icons.css"
      rel="stylesheet"
    />
    <link href="../../../../../assets/vendor/aos/aos.css" rel="stylesheet" />
    <link
      href="../../../../../assets/vendor/glightbox/css/glightbox.min.css"
      rel="stylesheet"
    />
    <link
      href="../../../../../assets/vendor/swiper/swiper-bundle.min.css"
      rel="stylesheet"
    />

    <!-- Main CSS File -->
    <link href="../../../../../assets/css/main.css" rel="stylesheet" />
    <!--Highlight-->
    <link
      href="../../../../../assets/highlight/styles/github.css"
      rel="stylesheet"
    />

    <script
      type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    ></script>
  </head>

  <body>
    <!-- Page Title -->
    <div class="page-title dark-background">
      <div
        class="container d-lg-flex justify-content-between align-items-center"
      >
        <h1 class="mb-2 mb-lg-0">Singular Value Decomposiion</h1>
        <nav class="breadcrumbs">
          <ol>
            <li><a href="../../../../../index.html">Home</a></li>
            <li><a href="../../../../../blogs.html">Blogs</a></li>
          </ol>
        </nav>
      </div>
    </div>
    <!-- End Page Title -->

    <div id="wrap">
      <main id="content">
        <div class="container">
          <div class="row">
            <div class="col-md-12">
              <div id="body">
                <div class="info">
                  <p style="font-family: Roboto; font-size: 130%">
                    Posted on
                    <time datetime="2023-02-04">February 04, 2023</time>
                  </p>
                  <div class="tag-container">
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/neural_networks.html"
                        >neural networks</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/deep_learning.html"
                        >deep learning</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/transformers.html"
                        >transformers</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/attention.html"
                        >attention</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/nlp.html"
                        >NLP</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/machine_learning.html"
                        >machine learning</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/ai.html">AI</a></span
                    >
                  </div>
                </div>
                <section>
                  <subsection>
                    <p>
                      Singular Value Decomposition (SVD) is a fundamental matrix
                      factorization technique widely used in machine learning,
                      signal processing, and data analysis. It decomposes a
                      given matrix into three matrices:
                      <span class="math">\(U\)</span>,
                      <span class="math">\(\Sigma\)</span>, and
                      <span class="math">\(V\)</span>, where
                      <span class="math">\(U\)</span> and
                      <span class="math">\(V\)</span> are orthogonal matrices,
                      and <span class="math">\(\Sigma\)</span> is a diagonal
                      matrix.
                    </p>
                    <span class="math">\[A = U \Sigma V^T\]</span>
                    <div class="bigcenterimgcontainer">
                      <img src="img/svd.jpg" alt style />
                    </div>
                    <div class="spaceafterimg"></div>
                    <p>
                      The elements of <span class="math">\(\Sigma\)</span> are
                      called the singular values of
                      <span class="math">\(A\)</span>.
                    </p>
                    <p>
                      The <span class="math">\(U\)</span> matrix contains the
                      left singular vectors of the matrix
                      <span class="math">\(A\)</span>. These singular vectors
                      are orthogonal to each other and span the column space of
                      <span class="math">\(A\)</span>. The number of columns in
                      <span class="math">\(U\)</span> is equal to the number of
                      rows in <span class="math">\(A\)</span>.
                    </p>
                    <p>
                      The <span class="math">\(\Sigma\)</span> matrix is a
                      diagonal matrix that contains the singular values of
                      <span class="math">\(A\)</span> on the diagonal. The
                      singular values are sorted in descending order. The
                      singular values are positive real numbers and they measure
                      the importance or magnitude of each singular vector.
                    </p>
                    <p>
                      The <span class="math">\(V\)</span> matrix contains the
                      right singular vectors of <span class="math">\(A\)</span>.
                      These singular vectors are orthogonal to each other and
                      span the row space of <span class="math">\(A\)</span>. The
                      number of columns in <span class="math">\(V\)</span> is
                      equal to the number of columns in
                      <span class="math">\(A\)</span>.
                    </p>
                  </subsection>
                  <subsection>
                    <h2>Properties of SVD</h2>
                    <ol>
                      <li>
                        Rank: The rank of a matrix is equal to the number of
                        non-zero singular values in the
                        <span class="math">\(\Sigma\)</span> matrix.
                      </li>
                      <li>
                        Orthogonality: The matrices
                        <span class="math">\(A\)</span> and
                        <span class="math">\(A^T\)</span> are both orthogonal
                        matrices, meaning that their transposes are also their
                        inverses. The matrix
                        <span class="math">\(\Sigma\)</span> is a diagonal
                        matrix.
                      </li>
                      <li>
                        Pseudoinverse: The Moore-Penrose pseudoinverse (a form
                        of SVD) can be used to compute the pseudoinverse of a
                        matrix, which is a generalization of the inverse matrix
                        to non-square matrices. The pseudoinverse can be used to
                        solve linear equations that are not solvable using the
                        standard inverse. The Moore-Penrose pseudoinverse is
                        given by the equation:
                        <span class="math">\[A^+ = V \Sigma^+ U^T\]</span>,
                        where <span class="math">\(\Sigma^+\)</span> is obtained
                        by taking the reciprocal of nonzero singular values in
                        <span class="math">\(\Sigma\)</span> and transposing it.
                        It provides the least-squares solution to
                        underdetermined or overdetermined systems.
                      </li>
                      <li>
                        Matrix Factorization: SVD can be used for matrix
                        factorization, which is the process of representing a
                        matrix as the product of two or more lower-dimensional
                        matrices. This factorization can be useful for reducing
                        the computational complexity of algorithms, improving
                        the interpretability of the data, and discovering
                        meaningful structure in the data.
                      </li>
                      <li>
                        Low-Rank Approximations: SVD can be used to find
                        low-rank approximations of matrices. The low-rank
                        approximation is given by the equation:
                        <span class="math"
                          >\[A_k \approx U_k \Sigma_k V_k^T\]</span
                        >This involves retaining only the top
                        <span class="math">\(k\)</span> singular values and
                        corresponding singular vectors, where
                        <span class="math">\(k\)</span> is much smaller than the
                        dimensions of the original matrix, it is possible to
                        obtain a rank-<span class="math">\(k\)</span>
                        approximation of the matrix (giving an optimal
                        approximation under Frobenius norm) that captures most
                        of its essential structure. This reduces the
                        computational cost of SVD.
                      </li>
                      <li>
                        Robustness: SVD is a robust technique that is not
                        affected by outliers and is able to preserve the
                        important information in the data. This makes it useful
                        in fields such as image processing and data analysis,
                        where outliers and noise are common.
                      </li>
                      <li>
                        Computational complexity: The computational complexity
                        of SVD for a <span class="math">\(m x n\)</span> matrix
                        is <span class="math">\(O(\min(mn^2,m^2n))\)</span>.
                      </li>
                    </ol>
                  </subsection>
                  <subsection>
                    <h2>Types of SVD</h2>
                    <p>
                      While the standard SVD decomposes a matrix into three
                      components, different variations optimize for specific use
                      cases, computational efficiency, or scalability. Below are
                      the most important types of SVD:
                    </p>
                    <ol>
                      <li>
                        <b>Truncated SVD</b>
                        <p>
                          Truncated SVD is a variation of SVD that involves
                          keeping only the top k singular values and
                          corresponding singular vectors. This reduces the
                          computational cost of SVD while retaining the most
                          important information of the original matrix. It is
                          important to note, that first it computes all the
                          singular values of
                          <span class="math">\(\Sigma\)</span>, and only then it
                          selects the top
                          <span class="math">\(k\)</span> components based on
                          some pre-determined threshold. It is primarily used
                          for dimensionality reduction, noise reduction, and
                          feature extraction in text and image processing.
                        </p>
                      </li>
                      <li>
                        <b>Economical SVD</b>
                        <p>
                          The economical SVD (or compact SVD) is a variation of
                          SVD that involves computing only the first k singular
                          values and singular vectors directly, where k is much
                          smaller than the dimensions of the original matrix.
                          This reduces the computational cost of SVD for
                          matrices with a large number of columns. It is
                          important to note that although economical SVD sounds
                          similar to Truncated SVD, they are not the same. Here,
                          the sizes of the matrics
                          <span class="math">\(U\)</span> and
                          <span class="math">\(V\)</span> are reduced from
                          <span class="math">\( m~x~m \)</span> to
                          <span class="math">\(m~x~k\)</span> and
                          <span class="math">\( n~x~n \)</span> to
                          <span class="math">\(n~x~k\)</span> to avoid
                          computation when
                          <span class="math">\(m \neq n\)</span>. It is
                          particularly useful for large-scale matrices, where
                          memory efficiency is critical such as, data
                          compression, NLP, recommendation systems, and large
                          dataset analysis.
                        </p>
                      </li>
                      <li>
                        <b>Randomized SVD</b>
                        <p>
                          Randomized SVD is a variation of SVD that leverages
                          probabilistic techniques, such as random projections,
                          to approximate singular values faster than traditional
                          methods. Instead of explicitly computing singular
                          values, it estimates them using random sampling and
                          Monte Carlo methods. It is particlarly useful for
                          streaming data, recommender systems, topic modeling,
                          graph embeddings and large-scale matrices where the
                          standard SVD computation is computationally expensive.
                        </p>
                      </li>
                      <li>
                        <b>Extensions of SVD</b>
                        <p>
                          SVD has been extended to more complex data structures,
                          such as tensors and graphs. Tensor SVD generalizes SVD
                          to higher-dimensional tensors instead of 2D matrices.
                          It is useful in computer vision, hyperspectral
                          imaging, and deep learning. Graph SVD applies SVD to
                          graph adjacency matrices for tasks such as community
                          detection, link prediction, and spectral clustering.
                        </p>
                      </li>
                    </ol>
                  </subsection>
                  <subsection>
                    <h2>Applications of SVD</h2>
                    <ol>
                      <li>
                        Dimensionality Reduction: SVD is often used for
                        dimensionality reduction by keeping only the k largest
                        singular values and corresponding singular vectors. This
                        reduces the size of the data while retaining most of its
                        important information.
                      </li>
                      <li>
                        Image Processing: SVD is used in image processing for
                        tasks such as image compression, denoising, and image
                        restoration. By removing the low-rank components of the
                        matrix (i.e., the small singular values and
                        corresponding singular vectors), we can remove the noise
                        from the data.
                      </li>
                      <li>
                        Recommender Systems: In recommender systems, SVD is used
                        to factorize the user-item interaction matrix into two
                        low-rank matrices, which can then be used to make
                        recommendations to users.
                      </li>
                      <li>
                        Natural Language Processing: SVD is used in natural
                        language processing for tasks such as text
                        classification, sentiment analysis, and topic modeling.
                      </li>
                      <li>
                        Latent Semantic Analysis: Latent semantic analysis is a
                        method for analyzing the relationships between a set of
                        documents and the terms they contain. SVD is used to
                        factorize the term-document matrix into two low-rank
                        matrices, which can then be used to identify the latent
                        topics in the documents.
                      </li>
                    </ol>
                  </subsection>
                  <subsection>
                    <h2>Connection with PCA</h2>
                    <p>
                      Unlike Eigenvalue Decomposiion, SVD applies to any matrix
                      (even rectangular). For symmetric matrices
                      <span class="math">\(A=A^T\)</span> the singular values of
                      SVD are the absolute values of eigenvalues of
                      <span class="math">\(A\)</span>. In general, PCA is
                      typically implemented using SVD on the covariance matrix
                      <span class="math">\(A^T A\)</span> rather than computing
                      eigenvalues directly. The principal components correspond
                      to the right singular vectors in
                      <span class="math">\(V\)</span>.
                    </p>
                    <div class="bigcenterimgcontainer">
                      <img src="img/pca.gif" alt style />
                    </div>
                  </subsection>
                  <subsection>
                    <h2>SVD Implentation</h2>
                    <h3>Full-SVD Implentation</h3>
                    <pre class="code-block"><code class="language-python">
import numpy as np

def svd_decomposition(A, tol=1e-10):
    """Computes Singular Value Decomposition (SVD) from scratch"""
    # Compute eigenvalues and eigenvectors of A^T A to get V
    AtA = np.dot(A.T, A)
    eig_vals_v, V = np.linalg.eigh(AtA)  # Eigenvalue decomposition

    # Sort eigenvalues and corresponding eigenvectors in descending order
    sorted_indices = np.argsort(eig_vals_v)[::-1]
    eig_vals_v = eig_vals_v[sorted_indices]
    V = V[:, sorted_indices]

    # Compute singular values as square roots of eigenvalues
    singular_values = np.sqrt(np.maximum(eig_vals_v, 0))  # Ensure non-negative values

    # Compute eigenvalues and eigenvectors of A A^T to get U
    AAt = np.dot(A, A.T)
    eig_vals_u, U = np.linalg.eigh(AAt)

    # Sort eigenvalues and corresponding eigenvectors in descending order
    sorted_indices = np.argsort(eig_vals_u)[::-1]
    U = U[:, sorted_indices]

    # Construct Σ (S matrix) as a diagonal matrix of singular values
    S = np.zeros_like(A, dtype=float)
    min_dim = min(A.shape)
    S[:min_dim, :min_dim] = np.diag(singular_values[:min_dim])

    return U, S, V.T
                    </code></pre>

                    <p>The code can be broken down into the following steps</p>
                    <ol>
                      <li>
                        We find eigenvalues & eigenvectors of
                        <span class="math">\(A^T.A\)</span> to get
                        <span class="math">\(V\)</span>
                      </li>
                      <li>
                        We find eigenvalues & eigenvectors of
                        <span class="math">\(A.A^T\)</span> to get
                        <span class="math">\(U\)</span>
                      </li>
                      <li>
                        The eigenvalues must then be sorted in descending order
                        to align with the standard SVD format.
                      </li>
                      <li>
                        The singular values are the square roots of eigenvalues
                        of <span class="math">\(A^T.A\)</span>
                      </li>
                      <li>
                        S (representing <span class="math">\(\Sigma\)</span>) is
                        a diagonal matrix where the top-left
                        <span class="math">\(k~x~k\)</span> portion contains
                        singular values.
                      </li>
                      <li>
                        Finally, we reconstruct
                        <span class="math">\(A\)</span> using
                        <span class="math">\(U \Sigma V^T\)</span> to ensure
                        correctness
                      </li>
                    </ol>
                    <p>This is called as</p>
                    <pre class="code-block"><code class="language-python">
==========================
FUNCTION CALL:
==========================
                        
A = np.array([[1., -2., 5.], [2., 3., 1.], [3., 8., -3.]])
U, S, Vt = svd_decomposition(A)
# Print results
print("U Matrix:\n", U)
print("S Matrix:\n", S)
print("V Transposed Matrix:\n", Vt)

==========================
OUTPUTS:
==========================

U Matrix:
 [[-0.3489058  -0.84356273 -0.40824829]
 [ 0.27369733 -0.50835333  0.81649658]
 [ 0.89630046 -0.17314393 -0.40824829]]
S Matrix:
 [[1.00570603e+01 0.00000000e+00 0.00000000e+00]
 [0.00000000e+00 4.98553285e+00 0.00000000e+00]
 [0.00000000e+00 0.00000000e+00 7.01884803e-08]]
V Transposed Matrix:
 [[-0.28710082 -0.86400071  0.41361321]
 [ 0.47732133  0.24532702  0.84378848]
 [ 0.83050435 -0.43967877 -0.34197238]]

==========================
RECONSTRUCTION:
==========================
# Verify reconstruction
A_reconstructed = U @ S @ Vt
print("\nReconstructed A:\n", A_reconstructed)
assert np.allclose(A.astype(float), -A_reconstructed)

==========================
OUTPUTS:
==========================
Reconstructed A:
 [[-1.00000002  2.00000001 -4.99999999]
 [-1.99999995 -3.00000003 -1.00000002]
 [-3.00000002 -7.99999999  3.00000001]]
                        </code></pre>
                    <p>
                      The code above computes a full-SVD using Eigenvalue
                      decomposition, calculating all singular values and
                      vectors. Another slight variation of SVD which computes
                      the full set of singular values, but does not stores the
                      unnecessary zeros is called Economical or Compact SVD.
                      This leads to certain memory optimization, as the size of
                      matrices reduces a bit. For instance, if
                      <span class="math">\(A\)</span> is of shape
                      <span class="math">\(m~x~n, m \gt n\)</span>, then full
                      svd gives <span class="math">\(U = m~x~m\)</span>,
                      <span class="math">\(\Sigma = m~x~n\)</span> and
                      <span class="math">\(V^T =n~x~n\)</span>. Whereas
                      economical SVD produces
                      <span class="math">\(U = m~x~n\)</span>,
                      <span class="math">\(\Sigma = n~x~n\)</span> and
                      <span class="math">\(V^T =n~x~n\)</span>. It can be
                      implemented in a very similar way as full SVD but using a
                      masking to remove the zeros as:
                    </p>
                    <h3>Economical-SVD Implentation</h3>
                    <pre class="code-block"><code class="language-python">
def economical_svd(A):
    """ Compute the economical SVD (compact SVD) of matrix A without using np.linalg.svd """
    m, n = A.shape

    # Compute A^T * A
    AtA = A.T @ A

    # Compute eigenvalues & eigenvectors of A^T * A
    eigvals, V = np.linalg.eigh(AtA)  # Using eigh since A^T A is symmetric

    # Sort eigenvalues and eigenvectors in descending order
    idx = np.argsort(eigvals)[::-1]  # Get indices of sorted eigenvalues
    eigvals = eigvals[idx]
    V = V[:, idx]

    # Compute singular values (square root of eigenvalues)
    S = np.sqrt(eigvals)

    # Remove zero or negative singular values (numerical precision issue)
    nonzero_mask = S > 1e-10  # Ignore near-zero singular values
    S = S[nonzero_mask]
    V = V[:, nonzero_mask]

    # Compute U = A * V * S⁻¹
    U = A @ V @ np.linalg.inv(np.diag(S))

    return U, np.diag(S), V.T  # Economical SVD (Compact form)                            
                        </code></pre>
                    <p>
                      This is also an expensive method as it has time-complexity
                      of
                      <span class="math">\(O(n^3)\)</span>. This might not
                      always be feasible. We can compute the top
                      <span class="math">\(k\)</span>-singular values without
                      computing all the eigenvalues of
                      <span class="math">\(A.A^T\)</span> or
                      <span class="math">\(A^T.A\)</span>.  This method is
                      called as Truncated SVD and can be implemented as below:
                    </p>
                    <h3>Truncated-SVD Implentation</h3>
                    <pre class="code-block"><code class="language-python">
def truncated_svd(A, k, tol=1e-10, max_iter=100):
    """Computes Truncated SVD using Power Iteration"""
    m, n = A.shape
    U = np.zeros((m, k))
    S = np.zeros((k, k))
    V = np.zeros((n, k))

    A_approx = A.copy() # Copy of A to iteratively deflate it

    for i in range(k):
        # Random initialization of a vector
        v = np.random.rand(n, 1)
        v /= np.linalg.norm(v)

        # Power iteration to approximate top singular vector
        for _ in range(max_iter):
            Av = np.dot(A_approx, v)
            u = Av / np.linalg.norm(Av)
            AvT = np.dot(A_approx.T, u)
            v_new = AvT / np.linalg.norm(AvT)

            # Convergence check
            if np.linalg.norm(v - v_new) < tol:
                break
            v = v_new

        # Singular value estimation
        sigma = np.linalg.norm(np.dot(A_approx, v))

        # Store computed singular vector and value
        U[:, i] = u.flatten()
        S[i, i] = sigma
        V[:, i] = v.flatten()

        # Deflate A to remove extracted singular component
        A_approx -= sigma * np.outer(u, v.T)

    return U, S, V.T                    
                    </code></pre>

                    <p>The code can be broken down into the following steps</p>
                    <ol>
                      <li>
                        We initialize matrices
                        <span class="math">\(U (m~x~k)\)</span> stores the top
                        <span class="math">\(k\)</span> left singular vectors,
                        <span class="math">\(S (k~x~k)\)</span> stores the
                        singular values in a diagonal matrix,
                        <span class="math">\(k~x~k\)</span>, and
                        <span class="math">\(V (n~x~k)\)</span> stores the top
                        <span class="math">\(k\)</span> right singular vectors.
                      </li>
                      <li>
                        We compute the top
                        <span class="math">\(k\)</span> singular values and
                        vectors iteratively. Each iteration finds one singular
                        value and its corresponding singular vectors.
                      </li>
                      <li>
                        We randomly initialize a right singular vector
                        <span class="math">\(v\)</span> of size
                        <span class="math">\((n, 1)\)</span>, and set it to have
                        unit norm.
                      </li>
                      <li>
                        Power iteration step:
                        <ol>
                          <li>
                            Multiply <span class="math">\(A\)</span> with
                            <span class="math">\(v\)</span> to get approximate
                            left singular vector
                            <span class="math">\(u\)</span>.
                          </li>
                          <li>
                            Multiply <span class="math">\(A^T\)</span>
                            with
                            <span class="math">\(u\)</span> to refine
                            <span class="math">\(v\)</span> .
                          </li>
                          <li>Normalize at each step to maintain unit norm.</li>
                          <li>
                            Repeat until convergence (i.e.,
                            <span class="math">\(v\)</span>
                            stops changing significantly)
                          </li>
                        </ol>
                      </li>
                      <li>
                        S (representing <span class="math">\(\Sigma\)</span>) is
                        a diagonal matrix of size
                        <span class="math">\(k~x~k\)</span> containing the
                        singular values.
                      </li>
                      <li>
                        Finally, we approximate
                        <span class="math">\(A\)</span> using
                        <span class="math">\(U \Sigma V^T\)</span> to ensure
                        correctness
                      </li>
                    </ol>
                    <pre class="code-block"><code class="language-python">
==========================
FUNCTION CALL:
==========================
                        
k = 2  # Compute only top 2 singular values/vectors
U, S, Vt = truncated_svd(A, k)

# Print results
print("U Matrix:\n", U)
print("S Matrix:\n", S)
print("V Transposed Matrix:\n", Vt)

==========================
OUTPUTS:
==========================

U Matrix:
 [[-0.3489058   0.84356273]
 [ 0.27369733  0.50835333]
 [ 0.89630046  0.17314393]]
S Matrix:
 [[10.05706032  0.        ]
 [ 0.          4.98553285]]
V Transposed Matrix:
 [[ 0.28710082  0.86400071 -0.41361321]
 [ 0.47732133  0.24532702  0.84378848]]

==========================
RECONSTRUCTION:
==========================
# Verify reconstruction
A_approx = U @ S @ Vt
print("\nApproximated A (Truncated):\n", A_approx)
assert np.allclose(A.astype(float), -A_approx)

==========================
OUTPUTS:
==========================
Approximated A (Truncated):
 [[ 1. -2.  5.]
 [ 2.  3.  1.]
 [ 3.  8. -3.]]
                        </code></pre>
                    <p>
                      Another optimized implementation called Randomized SVD
                      (RSVD) is particlarly useful for large-scale matrices.
                      Instead of computing the full-SVD, it uses random
                      projections to find a low-dminesional subspace that
                      captures most of the variance. Since it operates on this
                      subspace only, it is computationally efficient. Let's look
                      at the implemenmtation of this:
                    </p>
                    <h3>Randomized-SVD Implentation</h3>
                    <pre class="code-block"><code class="language-python">
import numpy as np

def randomized_svd(A, k, num_power_iterations=2):
    """
    Computes an approximate SVD using a randomized method.
    
    Parameters:
    A (ndarray): Input matrix of shape (m, n)
    k (int): Target rank (number of singular values to keep)
    num_power_iterations (int): Number of power iterations for stability (default: 2)
    
    Returns:
    U (ndarray): Approximate left singular vectors (m x k)
    S (ndarray): Approximate singular values (k x k)
    Vt (ndarray): Approximate right singular vectors (k x n)
    """
    m, n = A.shape
    
    # Step 1: Generate a random Gaussian matrix Omega (n x k)
    Omega = np.random.randn(n, k)  
    
    # Step 2: Compute Y = A * Omega
    Y = A @ Omega  
    
    # Step 3: Power Iterations (Optional: Improves accuracy)
    for _ in range(num_power_iterations):
        Y = A @ (A.T @ Y)
    
    # Step 4: QR Decomposition to get orthonormal basis Q
    Q, _ = np.linalg.qr(Y)  
    
    # Step 5: Compute B = Q^T * A (reduced matrix)
    B = Q.T @ A  
    
    # Step 6: Compute economical SVD on small matrix B
    U_tilde, S, Vt = np.linalg.svd(B, full_matrices=False)
    
    # Step 7: Compute U = Q * U_tilde
    U = Q @ U_tilde  
    
    return U, np.diag(S), Vt

# Example: Randomized SVD on a large 1000x500 matrix
A = np.random.randn(1000, 500)  # Large random matrix
k = 10  # Target rank (only keep 10 singular values)

U, S, Vt = randomized_svd(A, k)

# Print shapes to verify correctness
print("U shape:", U.shape)  # (1000, 10)
print("S shape:", S.shape)  # (10, 10)
print("Vt shape:", Vt.shape)  # (10, 500)

                        </code></pre>
                    <p>
                      The above implementation used all the concepts we have
                      learnt so far, in addition to new concepts. The way the
                      code works is:
                    </p>
                    <ol>
                      <li>
                        Generate a random Gaussian matrix
                        <span class="math"> \(\Omega (n x k)\) </span>.
                      </li>
                      <li>
                        Project <span class="math">\(A\)</span> onto a
                        lower-dimensional space and compute
                        <span class="math">\(Y = A \Omega\)</span>.
                      </li>
                      <li>
                        Perform Power-iteration to improve accuracy by refining
                        <span class="math">\(Y\)</span>.
                      </li>
                      <li>
                        Orthogonalize <span class="math">\(Y\)</span> using QR
                        decomposition to get orthonormal basis
                        <span class="math">\(Q\)</span>.
                      </li>
                      <li>
                        Reduce <span class="math">\(A\)</span> to a reduced
                        matrix <span class="math">\(B = Q^T A\)</span> of shape
                        <span class="math">\(m x k\)</span>.
                      </li>
                      <li>
                        Perform economical SVD on
                        <span class="math">\(B\)</span> to get
                        <span class="math">\( \tilde{U} \Sigma V^T\)</span>
                      </li>
                      <li>
                        Finally compute the left singular vectors
                        <span class="math">\(U = Q \tilde{U}\)</span>
                      </li>
                    </ol>
                  </subsection>
                </section>
                <section>
                  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
                  <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
                </section>
              </div>
              <div class="col-md-4" id="toc-wrapper"></div>
            </div>
          </div>
        </div>
      </main>
    </div>

    <footer id="footer" class="footer position-relative light-background">
      <div class="container">
        <div class="copyright text-center">
          <p>
            © <span>Copyright</span>
            <strong class="px-1 sitename">Debanjan Saha</strong>
            <span>All Rights Reserved</span>
          </p>
        </div>
        <div class="credits">
          <!-- All the links in the footer should remain intact. -->
          <!-- You can delete the links only if you've purchased the pro version. -->
          <!-- Licensing information: https://bootstrapmade.com/license/ -->
          <!-- Purchase the pro version with working PHP/AJAX contact form: [buy-url] -->
          Designed by
          <a href="https://bootstrapmade.com/">BootstrapMade</a> Distributed by
          <a href="https://themewagon.com">ThemeWagon</a>
        </div>
      </div>
    </footer>

    <!-- Scroll Top -->
    <a
      href="#"
      id="scroll-top"
      class="scroll-top d-flex align-items-center justify-content-center"
      ><i class="bi bi-arrow-up-short"></i
    ></a>

    <!-- jQuery-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
    <script src="../../../../../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="../../../../../assets/js/main.js"></script>

    <script src="../../../../../assets/highlight/highlight.pack.js"></script>
    <script>
      hljs.initHighlightingOnLoad();
    </script>

    <script src="../../../../../assets/js/footnotes.js"></script>
    <script src="../../../../../assets/js/bootstrap-carousel.js"></script>
    <script src="../../../../../assets/js/inlineDisqussions.js"></script>
    <script src="../../../../../assets/js/toc.js"></script>

    <script
      type="text/javascript"
      src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    ></script>

    <noscript
      >Enable JavaScript for footnotes, Disqus comments, and other cool
      stuff.</noscript
    >
  </body>
</html>
