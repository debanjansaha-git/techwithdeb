<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <title>Tech with Deb</title>
    <meta content="" name="description" />
    <meta content="" name="keywords" />

    <!-- Favicons -->
    <link href="../../../../../assets/img/favicon.png" rel="icon" />
    <link
      href="../../../../../assets/img/apple-touch-icon.png"
      rel="apple-touch-icon"
    />

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Raleway:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap"
      rel="stylesheet"
    />

    <!-- Vendor CSS Files -->
    <link
      href="../../../../../assets/vendor/bootstrap/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <link
      href="../../../../../assets/vendor/bootstrap-icons/bootstrap-icons.css"
      rel="stylesheet"
    />
    <link href="../../../../../assets/vendor/aos/aos.css" rel="stylesheet" />
    <link
      href="../../../../../assets/vendor/glightbox/css/glightbox.min.css"
      rel="stylesheet"
    />
    <link
      href="../../../../../assets/vendor/swiper/swiper-bundle.min.css"
      rel="stylesheet"
    />

    <!-- Main CSS File -->
    <link href="../../../../../assets/css/main.css" rel="stylesheet" />
    <!--Highlight-->
    <link
      href="../../../../../assets/highlight/styles/github.css"
      rel="stylesheet"
    />

    <script
      type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    ></script>
  </head>

  <body>
    <!-- Page Title -->
    <div class="page-title dark-background">
      <div
        class="container d-lg-flex justify-content-between align-items-center"
      >
        <h1 class="mb-2 mb-lg-0">Understanding Transformers</h1>
        <nav class="breadcrumbs">
          <ol>
            <li><a href="../../../../../index.html">Home</a></li>
            <li><a href="../../../../../blogs.html">Blogs</a></li>
          </ol>
        </nav>
      </div>
    </div>
    <!-- End Page Title -->

    <div id="wrap">
      <main id="content">
        <div class="container">
          <div class="row">
            <div class="col-md-12">
              <div id="body">
                <div class="info">
                  <p style="font-family: Roboto; font-size: 100%">
                    Posted:
                    <time datetime="2023-09-30">September 30, 2023</time> <br />
                    Last Updated:
                    <time datetime="2025-02-28">February 26, 2025</time>
                  </p>
                  <div class="tag-container">
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/neural_networks.html"
                        >Neural Networks</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/deep_learning.html"
                        >Deep Learning</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/transformers.html"
                        >Transformers</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/attention.html"
                        >Attention</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/nlp.html"
                        >NLP</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/machine_learning.html"
                        >Machine Learning</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/ai.html">AI</a></span
                    >
                  </div>
                </div>
                <section>
                  <p>
                    The Transformer architecture has ushered in a seismic shift
                    in the ever-evolving world of Natural Language Processing
                    (NLP). Imagine this: The titans of NLP, LLMs such as ChatGPT
                    and BARD (now Gemini), have reshaped the way we engage with
                    language. They excel in Natural Language Understanding
                    (NLU), process unrivalled text completion skills, and
                    astound with their text summarization prowess. In the heart
                    of the transformer architecture lies the attention mechanism
                    which has revolutionized the way we look at natural
                    language. Attention as the name suggests is basically a
                    communication mechanism which decides how much attention
                    needs to be given to which parts of the sentences (tokens).
                  </p>
                  <div class="bigcenterimgcontainer">
                    <img src="img/attention-mechanism.png" alt style />
                  </div>
                  <div class="spaceafterimg"></div>
                  <p>
                    Think of attention mechanism as the highlighter in the above
                    image which quickly finds out what the most important
                    sections of a given document. The Transformer architecture
                    serves as the unifying thread that weaves through these
                    extraordinary LLMs. It‚Äôs no longer merely a buzzword; it
                    forms the bedrock of AI and Machine Learning. In this
                    article, we‚Äôll demystify LLMs from a layman‚Äôs perspective
                    and unravel the core principles that underpin Transformers.
                  </p>
                  <p>
                    Prepare to be captivated as we delve into the inner workings
                    of Transformers and unveil the mathematical wizardry
                    empowering them using a commonplace setting to reshape the
                    NLP landscape. Welcome to the realm of the Transformers,
                    where the future of language understanding and generation
                    awaits your exploration. üöÄüìñ
                  </p>

                  <h2>What is a Transformer?</h2>
                  <p>
                    The Transformer architecture was first introduced by Vaswani
                    et al, 2017 [1], a former senior staff research scientist at
                    Google Brain team in their paper ‚ÄúAttention Is All You Need‚Äù
                    which first introduced the concept of ‚Äúmulti-head attention‚Äù
                    mechanism to the rest of the world. Jacob Uszkoreit coined
                    the term ‚ÄúTransformers‚Äù.
                  </p>
                  <div class="centerimgcontainer">
                    <img src="img/transformer_cover.jpg" alt style />
                  </div>
                  <div class="spaceafterimg"></div>
                  <p>
                    Transformers is basically an architecture which uses various
                    types of attention mechanisms (self-attention, multi-head
                    attention, masked multi-head attention) replacing the
                    traditional sequential models such as convolutional neural
                    networks (CNNs) and recurrent neural networks (RNNs)
                    completely. They not only take lesser time to train, lesser
                    carbon footprint and achieve state-of-the-art results on
                    various benchmarks.
                  </p>

                  <h3>What problems did Transformers solve?</h3>
                  <p>
                    Transformers became a cornerstone in the field of AI/ML
                    because it solved a couple of rather difficult problems with
                    ease, namely:
                  </p>
                  <ol>
                    <li>
                      Mastering Long-Range Dependencies: Prior to Transformers,
                      the AI world struggled with sequential models such as
                      Long-Short Term Memory (LSTMs) and Gated Recurrent Units
                      (GRUs) to handle complex sequences. These models performed
                      well during training but struggled when faced with wide,
                      complicated sequences. Transformers, on the other hand,
                      breezed passed this hurdle thanks to their self-attention
                      magic.
                    </li>
                    <li>
                      The Parallelization Revolution: By utilising parallel
                      processing, transformers sparked a computational
                      revolution. While classic sequential models struggled
                      under the weight of compute complexity, Transformers
                      welcomed parallelization with ease, substantially reducing
                      processing times.
                    </li>
                    <li>
                      Massive Multitask Language Understanding (MMLU) Mastery:
                      Transformers have won the title of MMLU champions.
                      Traditional models were left in the dust by their natural
                      capacity to interpret complicated language tasks in
                      zero-shot or few-shot conditions. Tasks involving
                      intricate language understanding, translation, and other
                      complexities were handled with unrivalled dexterity.
                    </li>
                    <li>
                      Dominating NLP Arenas: Transformers have emerged as the
                      indisputable champions of NLP. They beat their
                      predecessors in every NLP domain imaginable, including
                      text generation, language translation, content
                      summarization, sentiment analysis, named entity
                      recognition (NER), and multilingual comprehension.
                    </li>
                    <li>
                      Beyond Text ‚Äî Multimodal Prowess: Transformers broke free
                      from the confines of text data. Their prowess extended
                      into diverse realms, from crafting captivating image
                      captions to deciphering spoken words with speech
                      recognition. The world of multimodal behavior bowed before
                      Transformers‚Äô versatility, paving the way for an array of
                      groundbreaking applications.
                    </li>
                  </ol>
                  <p>
                    While this list merely scratches the surface of
                    Transformers‚Äô conquests, it‚Äôs a testament to their sheer
                    brilliance. Delve deeper, and you‚Äôll uncover an array of
                    problems they‚Äôve elegantly resolved, laying the foundation
                    for a new era in AI and machine learning.
                  </p>
                  <p>
                    Enough with the backdrop. Now, we‚Äôll uses an analogy to
                    examine the Transformer design from this angle. So sit back,
                    relax, and grab a cup of coffee as we explore the mysterious
                    world of Transformers.
                  </p>

                  <h2>Pre-requisites</h2>
                  <p>
                    Before diving head first into the world of Transformers, we
                    need to understand a few key concept - tokens, embeddings,
                    positional embeddings. So, what is a token? A token is
                    basically a numeric (vectorized) representation of a word or
                    sentence or text.
                  </p>
                  <p>
                    Unlike humans, computers cannot understand natural language.
                    So in order to make computers understand language and its
                    context, we transform the text into tokens. This process is
                    known as tokenization.
                  </p>
                  <p>
                    The tokens upto a fixed length (embedding dimension) are
                    combined to form a vector, called embeddings. There are many
                    ways to do this, but that is a topic for a different day.
                  </p>
                  <p>
                    The next important concept is how words are arranged in a
                    sentence. Consider the sentence
                    <q>She only told him the truth</q>. If we slightly alter the
                    position of the words as <q>She told him only the truth</q>.
                    While both sentences are similar, they mean different
                    things.
                  </p>
                  <p>
                    Hence, we can note that the position of words in a sentence
                    is very important, and this must be accurately captured. To
                    achieve this we calculate positional embeddings of the input
                    text, which basically capture the sequential information of
                    the text (the position at which a particular word appears in
                    the sentence).
                  </p>

                  <h2>Self-Attention Mechanism</h2>
                  <p>
                    Self-attention is at the core of Transformers, and in order
                    to fully understand Transformers, we need to first
                    understand what exactly is attention. The authors [1]
                    describe attention function as ‚Äúmapping a query and set of
                    key-value pairs to an output‚Äù. We will look at this more in
                    detail later but for now, let‚Äôs try to understand this with
                    a simple example.
                  </p>
                  <p>
                    Imagine you‚Äôre in a classroom, and the teacher has just
                    asked a fascinating question. Now, there are many eager
                    students with their hands raised, each hoping to share their
                    knowledge.
                  </p>
                  <div class="centerimgcontainer">
                    <img src="img/slf_attention_intuition.webp" alt style />
                  </div>
                  <div class="spaceafterimg"></div>
                  <p>
                    But here‚Äôs the cool part: the teacher‚Äôs attention isn‚Äôt
                    fixed on one student. It‚Äôs like they have magical glasses
                    that can adjust their focus. They look at each raised hand,
                    but they pay more attention to the students who seem to know
                    the answer best.
                  </p>
                  <p>
                    So, when a student confidently shoots their hand up and has
                    a strong response, it‚Äôs as if their hand shines the
                    brightest, and the teacher‚Äôs magical glasses zoom in on
                    them. The teacher listens carefully, giving extra attention
                    to that student‚Äôs explanation.
                  </p>
                  <p>
                    But the other students aren‚Äôt left out; their hands are
                    still up, and the teacher can quickly switch their magical
                    glasses to listen closely to someone else if they have
                    something valuable to add.
                  </p>
                  <p>
                    Here we need to understand three fundamental concepts of
                    attention mechanism: query, key and value.
                  </p>
                  <p>Query (Q) ‚Üí The teacher‚Äôs question.</p>
                  <p>
                    The teacher is looking for the best answer, so their query
                    is essentially:
                    <q
                      >Who in the class has the best response to my question?</q
                    >
                  </p>
                  <p>Keys (K) ‚Üí The students' understanding of the question.</p>
                  <p>
                    Each student has their own level of understanding of the
                    topic. Some students know the answer really well, while
                    others are unsure. These keys represent how relevant each
                    student‚Äôs knowledge is to the question.
                  </p>
                  <p>Values (V) ‚Üí The students' actual answers.</p>
                  <p>
                    Once the teacher identifies which students are the most
                    knowledgeable (by matching their query with the students‚Äô
                    keys), they then listen to the actual answers those students
                    provide. These answers represent the values.
                  </p>
                  <p>
                    Now, that we understand what are Q, K, V, let's try to
                    understand how attention works. In my opinion, the best way
                    to understand this is to look at the code of how it works.
                    Let's look at how we can implement this in code.
                  </p>
                  <pre class="code-block"><code class="language-python">
class SelfAttentionHead(nn.Module):
    def __init__(self, in_size, out_size):
        """
        in_size is embed_size
        out_size is head_size
        """
        super().__init__()
        self.head_size = out_size
        self.K = nn.Linear(in_size, self.head_size, bias=False)
        self.Q = nn.Linear(in_size, self.head_size, bias=False)
        self.V = nn.Linear(in_size, self.head_size, bias=False)
                    </code></pre>
                  <p>
                    The <spam class="code">SelfAttentionHead</spam> class
                    defines a self-attention mechanism, creating three
                    projection matrices: K (keys), Q (queries), and V (values).
                    These projections transform the input into lower-dimensional
                    representations of keys, queries, and values.
                  </p>
                  <pre class="code-block"><code class="language-python">
    def forward(self, x):
        keys = self.K(x)
        queries = self.Q(x)
        # affinities :
        # all the queries will dot-product with all the keys
        # transpose (swap) second dimension (input_length) with third (head_size)
        keys_t = keys.transpose(1, 2)
        autocorrs = (queries @ keys_t) * (self.head_size ** -0.5)  # (batch_size x input_length x input_length)
        '''
        (batch_size x input_length x embed_size) @ (batch_size x embed_size x input_length) ----> (batch_size x input_length x input_length)
        '''
                  </code></pre>
                  <p>
                    We swap the second and third dimensions of keys, to align
                    for matrix multiplication. Next, we compute the dot-product
                    of the queries and the keys.
                    <span class="math">\(Q \cdot K^T\)</span>. The dot-product
                    is scaled by
                    <span class="math"
                      >\(\frac{1}{\sqrt{\text{head_size}}} \)</span
                    >.to stabilize gradients
                  </p>
                  <pre class="code-block"><code class="language-python">
          autocorrs = torch.tril(autocorrs)
          autocorrs = autocorrs.masked_fill(autocorrs == 0, float('-inf'))
                  </code></pre>
                  <p>
                    Next, we mask future tokens to prevent them from influencing
                    the current token‚Äôs attention scores. This is achieved using
                    PyTorch's <spam class="code">tril</spam> function, which
                    zeroes out upper-triangular elements, followed by
                    masked_fill, setting them to negative infinity to prevent
                    influence during softmax computation.
                  </p>
                  <pre class="code-block"><code class="language-python">
          autocorrs = torch.softmax(autocorrs, dim=-1)
          values = self.V(x)  # (batch_size x input_length x head_size)
          out = autocorrs @ values
          return out
                </code></pre>

                  <p>
                    Finally, we pass it through a softmax function to obtain
                    attention weights and multiply them with the values matrix
                    to generate the final attention output. This is also known
                    as scaled dot-product attention scores.
                  </p>
                  <p>
                    In this way, just like in our classroom, self-attention
                    helps words in a sentence decide which other words to pay
                    more attention to. It‚Äôs like a teacher with magical focus,
                    ensuring everyone gets heard and understood!
                  </p>

                  <h2>Multi-Head Attention Mechanism</h2>
                  <p>
                    In our magical classroom, not only does the teacher have
                    those amazing adjustable focus glasses, but they also have
                    multiple pairs of eyes. Yes, it‚Äôs like they have a bunch of
                    eyes on their head, each with its own pair of glasses!
                  </p>
                  <p>
                    Now, when the teacher asks a question and students raise
                    their hands, these different pairs of eyes can focus on
                    different students simultaneously. It‚Äôs as if the teacher
                    can pay attention to multiple answers at once.
                  </p>
                  <div class="centerimgcontainer">
                    <img
                      src="img/multi_head_attention_intuition.webp"
                      alt
                      style
                    />
                  </div>
                  <div class="spaceafterimg"></div>
                  <p>
                    Imagine one pair of eyes zooms in on a student who‚Äôs great
                    with facts and figures, while another pair looks at a
                    student who‚Äôs fantastic at explaining things. Each pair of
                    eyes listens closely to a different aspect of the students‚Äô
                    answers.
                  </p>
                  <p>
                    Afterward, all these pairs of eyes come together, like a
                    team of experts sharing what they‚Äôve seen and heard. The
                    teacher then combines all this information to give a
                    comprehensive response or feedback to the class.
                  </p>
                  <p>
                    Let's try to understand this in greater detail by looking at
                    the code of how this can be implemented.
                  </p>
                  <pre class="code-block"><code class="language-python">
class MultiHeadAttention(nn.Module):
    """
    multiple parallel SA heads (communication among words)
    """

    def __init__(self, head_count, in_size, out_size):
        super().__init__()
        self.heads = nn.ModuleList(
            TransformerBlockLM.SelfAttentionHead(in_size, out_size // head_count)
            for _ in range(head_count)
        )
        self.layerNorm = nn.LayerNorm(out_size)
        # self.proj = nn.Linear(out_size, out_size)
                  </code></pre>
                  <p>
                    As we mentioned before, multi-head attention-mechanism is
                    simply a concatenation of multiple self-attention heads.
                    Each of these self-attention heads focusses on one or more
                    distinct aspect of the input and are mutually exclusive to
                    each other.
                  </p>
                  <p>
                    We create multiple instances of
                    <spam class="code">SelfAttentionHead</spam>, each operating
                    on a portion of the total embedding size
                    <spam class="code">(out_size // head_count)</spam>. Each
                    head works independently, attending to different aspects of
                    the input.
                  </p>
                  <p>
                    The outputs from all self-attention heads are concatenated
                    along the embedding dimension (dim=-1).
                  </p>
                  <p>
                    After concatenating the heads, we apply Layer Normalization
                    on the attention output to stabilize training and improve
                    performance. There has been quite some research on the
                    position of the LayerNorm. Finally We could apply a linear
                    projection <spam class="code">(self.proj)</spam> to mix
                    information across heads, but this is not strictly
                    necessary.
                  </p>
                  <pre class="code-block"><code class="language-python">
    def forward(self, x):
        # concat over channel/embeddings_size dimension
        return self.layerNorm(torch.cat([head(x) for head in self.heads], dim=-1))  # paper - after
        # return torch.cat([head(self.layerNorm(x)) for head in self.heads], dim=-1) # alternate - before
        # return self.proj(torch.cat([head(x) for head in self.heads], dim=-1))
                  </code></pre>
                  <p>
                    Post-normalization (default in the code): LayerNorm is
                    applied after attention heads are concatenated. This follows
                    the original Transformer paper.
                  </p>
                  <p>
                    Pre-normalization (alternative approach): LayerNorm is
                    applied before each self-attention head processes the input,
                    which can lead to more stable gradients.
                  </p>
                  <p>
                    So, in the world of language and Transformers, multi-head
                    attention works just like our teacher‚Äôs multiple pairs of
                    eyes. It helps the model focus on different parts of the
                    input at the same time, making it super sharp at
                    understanding various aspects of the text. It‚Äôs like having
                    a classroom full of experts, each looking at different
                    details and sharing their wisdom to solve complex language
                    problems!
                  </p>

                  <h2>Masked Multi-Head Attention Mechanism</h2>
                  <p>
                    Now, picture our classroom, but this time, the teacher is
                    giving a surprise quiz, and the students are answering by
                    raising their hands. However, there‚Äôs a twist ‚Äî some of the
                    students have a little paper mask on their hands!
                  </p>
                  <p>
                    These masked students have secret information they don‚Äôt
                    want to reveal to the rest of the class. So, when they raise
                    their hands, they only show their masks, hiding their actual
                    answers.
                  </p>
                  <div class="centerimgcontainer">
                    <img
                      src="img/masked_multi_head_attention_intuition.webp"
                      alt
                      style
                    />
                  </div>
                  <div class="spaceafterimg"></div>
                  <p>
                    Now, the teacher, equipped with their multiple pairs of eyes
                    and adjustable focus glasses, has a challenge. They need to
                    pay attention to all the raised hands, but they can‚Äôt see
                    what‚Äôs behind the masks of these secretive students.
                  </p>
                  <p>
                    So, the teacher‚Äôs eyes try to look past the masks, but they
                    can‚Äôt get a clear view of what‚Äôs underneath. They still
                    listen to these students, though, because they might share
                    some information without revealing their secret.
                  </p>
                  <p>
                    This is very similar to the masking approach we saw in
                    self-attention code. That was actually placed there to
                    handle masking in multi-head attention. Here, we replace
                    upper triangular values with -inf, so softmax treats them as
                    0 probability.
                  </p>
                  <p>
                    After that, we apply the
                    <spam class="code">torch.tril</spam> function to ensure that
                    each word only attends to itself and previous words. This is
                    essential as it prevents leakage of future tokens in
                    autoregressive models (GPT, transformers for text
                    generation). This is also called as causal language modeling
                    for decoding sequences. Finally, we can look at the code
                    which is a variant of the multi-head attention mechanism
                    using masked self-attention.
                  </p>
                  <pre class="code-block"><code class="language-python">
class MaskedMultiHeadAttention(nn.Module):
    """
    Multi-head attention with causal masking (prevents future tokens from influencing the current one).
    """

    def __init__(self, head_count, in_size, out_size):
        super().__init__()
        self.heads = nn.ModuleList(
            [SelfAttentionHead(in_size, out_size // head_count) for _ in range(head_count)]
        )
        self.layerNorm = nn.LayerNorm(out_size)

    def forward(self, x):
        # Create a causal mask: Lower triangular matrix of ones
        seq_len = x.shape[1]
        mask = torch.tril(torch.ones(seq_len, seq_len)).to(x.device)

        # Apply self-attention with masking
        attention_outputs = [head(x, mask) for head in self.heads]

        # Concatenate multiple attention head outputs
        out = torch.cat(attention_outputs, dim=-1)
        
        # Apply Layer Normalization
        return self.layerNorm(out)
                  </code></pre>
                  <p>
                    In the world of Transformers, masked multi-head attention
                    works similarly. It‚Äôs like some parts of the input data are
                    hidden behind masks, and the model needs to pay attention to
                    everything, even if it can‚Äôt see all the details. It‚Äôs
                    especially useful for tasks where you need to predict or
                    generate certain parts of the output while keeping other
                    parts hidden, just like our masked students in the classroom
                    quiz!
                  </p>

                  <h2>Conclusion</h2>
                  <p>
                    In the captivating world of Natural Language Processing
                    (NLP), the emergence of Transformers has been nothing short
                    of a revelation. These architectural wonders, inspired by
                    the magic of a dynamic classroom, have not only solved
                    longstanding challenges but have rewritten the rules of the
                    AI game.
                  </p>
                  <p>
                    We embarked on a journey to understand the core of
                    Transformers, much like eager students in that magical
                    classroom. Self-attention, akin to students‚Äô active
                    participation, showed us how words could dynamically adjust
                    their focus, leading to profound language comprehension.
                  </p>
                  <p>
                    Multi-head attention, reminiscent of the teacher‚Äôs multiple
                    pairs of eyes, allowed us to explore different aspects of
                    language simultaneously. It was like having a classroom full
                    of experts, each contributing their unique wisdom.
                  </p>
                  <p>
                    And then, we uncovered the intrigue of masked multi-head
                    attention, akin to secretive students with hidden knowledge.
                    Just as the teacher adapted to the challenge of obscured
                    answers, Transformers excel at deciphering information, even
                    when parts are concealed.
                  </p>
                  <p>
                    As we conclude our exploration, it‚Äôs evident that
                    Transformers have transformed NLP. They‚Äôve brought us
                    state-of-the-art language understanding, generation, and a
                    world of applications that were once beyond reach.
                  </p>
                  <p>
                    The future is bound to witness even greater feats as
                    Transformers continue to evolve, pushing the boundaries of
                    what‚Äôs possible in the realm of AI and machine learning. The
                    stage is set, and the magic of Transformers will continue to
                    dazzle and inspire. So, as we bid adieu to this adventure,
                    let‚Äôs remain ever curious and excited about what the world
                    of Transformers holds next. üöÄü§ñüìö
                  </p>
                  <p>Stiching it all together, here is an advanced implementation of Transformers from scratch using BPE tokenizer</p>
                  <pre class="code-block"><code class="language-python">
''' Use BPE Tokenizer with Decoder only Transformer
This model is an adaptation of the Transformer architecture that incorporates BPE tokenization
for efficient text encoding. It demonstrates an advanced usage of layer normalization and dropout
to improve model stability and performance.
@Author: Debanjan Saha
2024
+BPE
'''


class TransformerBlockLMNew(nn.Module):
    '''
    Transformer block applies multi-head attention followed by a feed-forward network
    Both stages include skip connections and are wrapped with layer normalization
    '''
    class TransformerBlock(nn.Module):
        def __init__(self, head_count, in_size, out_size, dropout=0.2):
            super().__init__()
            self.layerNorm1 = nn.LayerNorm(in_size)     # First layer normalization
            self.layerNorm2 = nn.LayerNorm(in_size)     # Second layer normalization
            # Multi-head attention mechanism
            self.comm = TransformerBlockLMNew.MultiHeadAttention(head_count=head_count,
                                                              in_size=in_size,
                                                              out_size=out_size,
                                                              dropout=dropout)
            # Feed-forward network
            self.ffwd = TransformerBlockLMNew.MLP(embed_size=out_size,
                                                dropout=dropout)

        def forward(self, x):
            '''Forward pass through one transformer block'''
            x = x + self.comm(self.layerNorm1(x))                               
            x = x + self.ffwd(self.layerNorm2(x))                               
            return x

    class MLP(nn.Module):
        '''
        The MLP consists of two linear transformations with a ReLU activation in between
        Dropout is applied as a regularizer
        FFNN (embed_size, embed_size*4, embed_size)
        '''
        def __init__(self, embed_size, dropout):
            super().__init__()
            self.mlp = nn.Sequential(nn.Linear(embed_size, embed_size * 4),
                                      nn.ReLU(),
                                      nn.Linear(embed_size * 4, embed_size),
                                      nn.Dropout(dropout))                       
            self.layerNorm = nn.LayerNorm(embed_size)       # Layer normalization

        def forward(self, x):  # think
            '''Forward pass through MLP'''
            return self.layerNorm(self.mlp(x))  # paper - after
            # return self.mlp(self.layerNorm(x)) # alternate - before

    class MultiHeadAttention(nn.Module):
        """
        multiple parallel SA heads (communication among words)
        MultiHeadAttention mechanism allows the model to jointly attend to information
        from different representation subspaces
        """

        def __init__(self, head_count, in_size, out_size, dropout):
            super().__init__()
            # Creating multiple attention heads
            self.heads = nn.ModuleList(
                TransformerBlockLMNew.SelfAttentionHead(in_size, out_size // head_count)
                for _ in range(head_count)
            )
            self.layerNorm = nn.LayerNorm(out_size)
            self.proj = nn.Linear(out_size, out_size)                           
            self.dropout = nn.Dropout(dropout)                                  

        def forward(self, x):
            '''Forward pass through multi-head attention'''
            # concat over channel/embeddings_size dimension
            x = self.layerNorm(x)                                               
            out =  self.layerNorm(torch.cat([head(x) for head in self.heads], dim=-1))  # paper - after
            # out = self.dropout(self.proj(out))                                  
            out = self.dropout(out)
            # return torch.cat([head(self.layerNorm(x)) for head in self.heads], dim=-1) # alternate - before
            # return self.proj(torch.cat([head(x) for head in self.heads], dim=-1))
            return out

    class SelfAttentionHead(nn.Module):
        '''SelfAttentionHead computes attention for a single head'''
        def __init__(self, in_size, out_size, dropout=0.2):
            """
            in_size is embed_size
            out_size is head_size
            """
            super().__init__()
            self.head_size = out_size       # Size of each attention head
            self.K = nn.Linear(in_size, self.head_size, bias=False)     # Key projection
            self.Q = nn.Linear(in_size, self.head_size, bias=False)     # Query projection
            self.V = nn.Linear(in_size, self.head_size, bias=False)     # Value projection
            self.dropout = nn.Dropout(dropout)                                  

        def forward(self, x):
            '''Forward pass through attention head'''
            keys = self.K(x)
            queries = self.Q(x)
            # affinities :
            # all the queries will dot-product with all the keys
            # transpose (swap) second dimension (input_length) with third (head_size)
            keys_t = keys.transpose(1, 2)
            # Compute dot product attention
            autocorrs = (queries @ keys_t) * (self.head_size ** -0.5)  # (batch_size x input_length x input_length)
            '''
            (batch_size x input_length x embed_size) @ (batch_size x embed_size x input_length) ----> (batch_size x input_length x input_length)
            '''
            autocorrs = torch.tril(autocorrs)       # Apply mask to force causality
            autocorrs = autocorrs.masked_fill(autocorrs == 0, float('-inf'))
            autocorrs = torch.softmax(autocorrs, dim=-1)
            autocorrs = self.dropout(autocorrs)                                 
            values = self.V(x)  # (batch_size x input_length x head_size)
            out = autocorrs @ values        # Apply attention to values
            return out

    def __init__(self, batch_size=4,
                  input_length=8,
                  embed_size=16,
                  sa_head_size=8,
                  sa_multihead_count=4,
                  n_layer = 6,                                                  
                  dropout=0.2,                                                   
                  vocab_size=256,                                                
                  num_merges=256,                                                
                  pos_embed=False,                                               # Flag for using positional embeddings
                  include_mlp=False):                                            # Flag to include MLP in the architecture
        super().__init__()
        self.blocks = None                                                      # Placeholder for Transformer blocks
        self.sa_heads = None                                                    # Placeholder for self-attention heads
        # sa_head_size head_size of self-attention module
        self.sa_head_size = sa_head_size                                        # Size of each self-attention head
        self.sa_multihead_count = sa_multihead_count                            # Number of self-attention heads
        self.linear_sahead_to_vocab = None                                      # Placeholder for linear projection from self-attention head outputs to vocabulary
        self.n_layer = n_layer                                                  # Number of Transformer blocks
        self.val_data = None                                                    # Placeholder for validation data
        self.train_data = None                                                  # Placeholder for training data
        self.val_text = None                                                    # Placeholder for validation text
        self.train_text = None                                                  # Placeholder for training text
        self.K = None                                                           # Placeholder for key matrix in self-attention mechanism
        self.token_embeddings_table = None                                      # Embedding table for token embeddings
        self.position_embeddings_table = None                                   # Embedding table for positional embeddings
        self.vocab = None                                                       # Vocabulary built from the training corpus
        self.vocab_size = vocab_size                                            # Size of the vocabulary
        self.is_pos_emb = pos_embed                                             # Flag indicating whether positional embeddings are used
        self.include_mlp = include_mlp                                          # Flag indicating whether an MLP is included after attention mechanisms
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'            # Device (GPU/CPU) on which the model is running
        # input_length = how many consecutive tokens/chars in one input
        self.input_length = input_length                                        # Length of input sequences
        # batch_size = how many inputs are going to be processed in-parallel (on GPU)
        self.batch_size = batch_size                                            # Size of batches during training
        # embed_size = embedding size
        self.embed_size = embed_size                                            # Dimensionality of token embeddings
        self.dropout = dropout                                                  # Global dropout rate

    def forward(self, in_ids, target=None):
        '''Forward method defines the computation performed at every call'''
        # Obtain token embeddings for input ids, possibly adding positional embeddings
        in_ids_emb = self.token_embeddings_table(in_ids[:, -self.input_length:])
        if self.is_pos_emb:
            # If positional embeddings are used, add them to the token embeddings
            in_ids_pos_emb = self.position_embeddings_table(
                torch.arange(in_ids[:, -self.input_length:].shape[1], device=self.device)
            )
            in_ids_emb = in_ids_emb + in_ids_pos_emb

        # print(f"Device - in_ids_emb: {in_ids_emb.device}, blocks parameters: {next(self.blocks.parameters()).device}, linear_sahead_to_vocab parameters: {next(self.linear_sahead_to_vocab.parameters()).device}")
        # Pass the embeddings through the Transformer blocks
        block_outputs = self.blocks(in_ids_emb).to(self.device)
        # Compute logits by projecting the output of the last Transformer block to the vocabulary space
        logits = self.linear_sahead_to_vocab(block_outputs)  # compute

        if target is None:
            ce_loss = None
        else:
            # If targets are provided, compute the cross-entropy loss
            batch_size, input_length, vocab_size = logits.shape
            logits_ = logits.view(batch_size * input_length, vocab_size)
            targets = target.view(batch_size * input_length)
            ce_loss = F.cross_entropy(logits_, targets)
        return logits, ce_loss

    def fit(self, train_iters=100, eval_iters=10, lr=0.0001):
        """
        Method to train the model
        train_iters = how many training iterations
        eval_iters = how many batches to evaluate to get average performance
        """
        optimizer = torch.optim.AdamW(self.parameters(), lr=lr)  # optimizer: AdamW (https://arxiv.org/abs/1711.05101)
        for iteration in range(train_iters):
            if iteration % eval_iters == 0:
                avg_loss = self.eval_loss(eval_iters)
                # print(f"iter {iteration}: train {avg_loss['train']} val {avg_loss['eval']}")
                print(f"iter {iteration}: Train ==> loss={avg_loss['train']['avg_loss'].item():.4f} perplexity={avg_loss['train']['perplexity']:.4f} \t Val ==> loss={avg_loss['eval']['avg_loss'].item():.4f} perplexity={avg_loss['eval']['perplexity']:.4f}")
            inputs, targets = self.get_batch(split='train')
            _, ce_loss = self(inputs, targets)
            optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step
            ce_loss.backward()  # propagate loss back to each unit in the network
            optimizer.step()  # update network parameters w.r.t the loss
        # torch.save(self, 'sa_pos_')

    def generate(self, context_token_ids, max_new_tokens):
        '''Method for generating text based on context token ids'''
        for _ in range(max_new_tokens):
            # Text generation using the model
            token_rep, _ = self(context_token_ids)
            last_token_rep = token_rep[:, -1, :]
            probs = F.softmax(last_token_rep, dim=1)
            next_token = torch.multinomial(probs, num_samples=1)
            context_token_ids = torch.cat((context_token_ids, next_token), dim=1)
        output_text = self.bpe_tokenizer.decode(context_token_ids[0].tolist())  
        return output_text

    @torch.no_grad()  # tell torch not to prepare for back-propagation (context manager)
    def eval_loss(self, eval_iters):
        '''Evaluation procedure'''
        perf = {}
        # set dropout and batch normalization layers to evaluation mode before running inference.
        self.eval()
        for split in ['train', 'eval']:
            total_loss = 0.0                                                    
            total_tokens = 0                                                    
            losses = torch.zeros(eval_iters)
            for k in range(eval_iters):
                tokens, targets = self.get_batch(split)  # get random batch of inputs and targete
                _, ce_loss = self(tokens, targets)  # forward pass
                total_loss += ce_loss.item() * tokens.size(0)                   
                total_tokens += tokens.size(0) * tokens.size(1)                
                losses[k] = ce_loss.item()  # the value of loss tensor as a standard Python number
            # Deb: Calculate average cross-entropy loss
            avg_loss = total_loss / (eval_iters * self.batch_size * self.input_length)
            # Deb: Calculate perplexity
            perplexity = math.exp(avg_loss)
            # perf[split] = losses.mean()
            perf[split] = {                                                     
                'avg_loss': losses.mean(),
                'perplexity': perplexity
            }
        self.train()  # turn-on training mode-
        return perf

    def prep(self, corpus):
        '''Method to prepare the model with training and validation data'''
        # Preprocessing and data preparation
        self.bpe_tokenizer = self.BPE(vocab_size=self.vocab_size)               
        self.bpe_tokenizer.train(corpus, num_merges=256)                        
        self.vocab = self.bpe_tokenizer.vocab                                   
        self.vocab_size = len(self.vocab)                                       

        n = len(corpus)
        self.train_text = corpus[:int(n * 0.9)]
        self.val_text = corpus[int(n * 0.9):]
        # Deb: Use BPE Tokenizer
        self.train_data = torch.tensor(self.bpe_tokenizer.encode(self.train_text), dtype=torch.long).to(self.device)
        self.val_data = torch.tensor(self.bpe_tokenizer.encode(self.val_text), dtype=torch.long).to(self.device)

        # look-up table for embeddings (vocab_size x embed_size)
        # it will be mapping each token id to a vector of embed_size
        # a wrapper to store vector representations of each token
        self.token_embeddings_table = nn.Embedding(self.vocab_size, self.embed_size)
        self.token_embeddings_table = self.token_embeddings_table.to(self.device)   

        if self.is_pos_emb:
            self.position_embeddings_table = nn.Embedding(self.input_length, self.embed_size)
            self.position_embeddings_table = self.position_embeddings_table.to(self.device)

        self.blocks = nn.Sequential(
            *[TransformerBlockLMNew.TransformerBlock(head_count=self.sa_multihead_count,
                                                  in_size=self.embed_size,
                                                  out_size=self.sa_head_size,
                                                  dropout=self.dropout) for _ in range(self.n_layer)]).to(self.device)
        # linear projection of sa_head output to vocabulary
        self.linear_sahead_to_vocab = nn.Linear(self.sa_head_size, self.vocab_size).to(self.device)


    def get_batch(self, split='train'):
        '''Method to generate a batch of training or validation data'''
        data = self.train_data if split == 'train' else self.val_data
        # get random chunks of length batch_size from data
        ix = torch.randint(len(data) - self.input_length,
                            (self.batch_size,))
        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])
        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])
        inputs_batch = inputs_batch.to(self.device)
        targets_batch = targets_batch.to(self.device)
        return inputs_batch, targets_batch

    class BPE:
        '''Deb uses Byte Pair Encoding instead of BiGram for sub-word tokenization
        instead of existing character level tokenization'''
        def __init__(self, vocab_size=256):
            self.vocab_size = vocab_size
            self.vocab = {chr(i): i for i in range(vocab_size)}  # Initial vocab of single characters
            self.merges = {}

        def train(self, text, num_merges):
            '''Training BPE on the corpus'''
            # Tokenize text into characters
            tokens = list(text)
            # Count frequency of pairs
            pairs = self.get_stats(tokens)

            for i in range(num_merges):
                if not pairs:
                    break
                # Find the most frequent pair
                best_pair = max(pairs, key=pairs.get)
                # Merge the best pair throughout the tokens
                tokens = self.merge_tokens(tokens, best_pair)
                # Update merges and vocab
                self.merges[best_pair] = i + self.vocab_size
                self.vocab[''.join(best_pair)] = i + self.vocab_size
                # Recount pairs
                pairs = self.get_stats(tokens)

        def get_stats(self, tokens):
            '''Calculating frequency of token pairs'''
            pairs = {}
            for i in range(len(tokens)-1):
                pair = (tokens[i], tokens[i+1])
                if pair in pairs:
                    pairs[pair] += 1
                else:
                    pairs[pair] = 1
            return pairs

        def merge_tokens(self, tokens, pair):
            '''Merging token pairs found during BPE training'''
            merged_tokens = []
            i = 0
            while i < len(tokens):
                # Merge pair if found, skip the next token as it's merged
                if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == pair:
                    merged_tokens.append(''.join(pair))
                    i += 2
                else:
                    merged_tokens.append(tokens[i])
                    i += 1
            return merged_tokens

        def encode(self, text):
            '''Encoding text using the trained BPE merges'''
            tokens = list(text)
            for pair in self.merges:
                tokens = self.merge_tokens(tokens, pair)
            return [self.vocab[token] for token in tokens if token in self.vocab]

        def decode(self, token_ids):
            '''Decoding token IDs back to text'''
            reverse_vocab = {idx: token for token, idx in self.vocab.items()}
            return ''.join([reverse_vocab[id_] for id_ in token_ids if id_ in reverse_vocab])
                
                  </pre></code>
                </section>
                <section>
                  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
                  <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
                  <h2>Recommended Reads</h2>
                  <ul>
                    <li>
                      <a href="https://arxiv.org/abs/1706.03762">
                        Attention Is All You Need (Paper) - Vaswani et al. 2017
                      </a>
                    </li>
                    <li>
                      <a
                        href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html"
                      >
                        Self-Attention from Scratch - Sebastian Raschka
                      </a>
                    </li>
                    <li>
                      <a
                        href="https://youtu.be/fjJOgb-E41w?si=W816q5tHQTN1YXlh"
                      >
                        Attention Mechanism Explainer Video - Google Cloud
                      </a>
                    </li>
                    <li>
                      <a
                        href="https://youtu.be/PSs6nxngL6k?si=_qt8f_58DtdQsg3S"
                      >
                        Attention Clearly Explained Video - Joshua Starmer
                        (StatQuest)
                      </a>
                    </li>
                  </ul>
                </section>
              </div>
              <div class="col-md-4" id="toc-wrapper"></div>
            </div>
          </div>
        </div>
      </main>
    </div>

    <footer id="footer" class="footer position-relative light-background">
      <div class="container">
        <div class="copyright text-center">
          <p>
            ¬© <span>Copyright</span>
            <strong class="px-1 sitename">Debanjan Saha</strong>
            <span>All Rights Reserved</span>
          </p>
        </div>
        <div class="credits">
          <!-- All the links in the footer should remain intact. -->
          <!-- You can delete the links only if you've purchased the pro version. -->
          <!-- Licensing information: https://bootstrapmade.com/license/ -->
          <!-- Purchase the pro version with working PHP/AJAX contact form: [buy-url] -->
          Designed by
          <a href="https://bootstrapmade.com/">BootstrapMade</a> Distributed by
          <a href="https://themewagon.com">ThemeWagon</a>
        </div>
      </div>
    </footer>

    <!-- Scroll Top -->
    <a
      href="#"
      id="scroll-top"
      class="scroll-top d-flex align-items-center justify-content-center"
      ><i class="bi bi-arrow-up-short"></i
    ></a>

    <!-- jQuery-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
    <script src="../../../../../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="../../../../../assets/js/main.js"></script>

    <script src="../../../../../assets/highlight/highlight.pack.js"></script>
    <script>
      hljs.initHighlightingOnLoad();
    </script>

    <script src="../../../../../assets/js/footnotes.js"></script>
    <script src="../../../../../assets/js/bootstrap-carousel.js"></script>
    <script src="../../../../../assets/js/inlineDisqussions.js"></script>
    <script src="../../../../../assets/js/toc.js"></script>

    <script
      type="text/javascript"
      src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    ></script>

    <noscript
      >Enable JavaScript for footnotes, Disqus comments, and other cool
      stuff.</noscript
    >
  </body>
</html>
