<!DOCTYPE html>
<html lang="en">

    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Understanding Transformers - Tech with Deb</title>
        
        <link rel="stylesheet" href="../../../../../fonts/Serif/cmun-serif.css" />
        <link rel="stylesheet" href="../../../../../fonts/Serif-Slanted/cmun-serif-slanted.css" />

        <!--BOOTSTRAP-->
        <link href="../../../../../bootstrap/css/bootstrap.min.css" rel="stylesheet">
        <!--mobile first-->
        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <!--removed html from url but still is html-->
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />

        <!--font awesome-->
        <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

        <!--fonts: allan & cardo-->
        <link href="http://fonts.googleapis.com/css?family=Droid+Serif" rel="stylesheet" type="text/css">
        <link href="http://fonts.googleapis.com/css?family=Droid+Sans" rel="stylesheet" type="text/css">

        <link href="../../../../../css/sticky-footer-navbar.css" rel="stylesheet">

        <link href="../../../../../css/default.css" rel="stylesheet">

        <link href="../../../../../css/bootstrap-carousel.css" rel="stylesheet">


        <link href="../../../../../css/inlineDisqussions.css" rel="stylesheet">

        <!--Highlight-->
        <link href="../../../../../highlight/styles/github.css" rel="stylesheet">
        
        <link href="../../../../../favicon.ico" rel="shortcut icon" />

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-49811703-1', 'colah.github.io');
  ga('require', 'linkid', 'linkid.js');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');

</script>


    </head>

    <body>
        <div id="wrap">
            <header>
                <nav class="navbar navbar-inverse navbar-static-top" role="navigation">
                    <div class="container">
                        <!--Toggle header for mobile-->
                        <div class="navbar-header">
                            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                                <span class="sr-only">Toggle navigation</span>
                                <span class="icon-bar"></span>
                                <span class="icon-bar"></span>
                                <span class="icon-bar"></span>
                            </button>
                            <a class="navbar-brand active" href="../../../../../" id="home">Tech with Deb</a>
                        </div>
                        <!--normal header-->
                        <div class="navbar-collapse collapse">
                            <ul class="nav navbar-nav navbar-right">
                                <li><a href="../../../../../"><span class="glyphicon glyphicon-pencil"></span>  Blog</a></li>
                                <li><a href="../../../../../about.html"><span class="glyphicon glyphicon-user"></span>  About</a></li>
                                <li><a href="../../../../../contact.html"><span class="glyphicon glyphicon-envelope"></span>  Contact</a></li>
                            </ul>
                        </div><!--/.nav-collapse -->
                    </div>
                </nav>
            </header>
            
            <main id="content">
                <div class="container">
                    <div class="row">
                        <div class="col-md-8">
                            <h1>Understanding Transformers</h1>
                            <div id="body">
                                <div class="info">
                                    <p style="font-family:CMSS; font-size:130%">Posted on <time datetime="2023-09-30">September 30, 2023</time></p>
                                    <div class="tag-container">
                                        <span class="tag-item"><a href="../../../../../posts/tags/neural_networks.html">neural networks</a></span>
                                        <span class="tag-item"><a href="../../../../../posts/tags/deep_learning.html">deep learning</a></span>
                                        <span class="tag-item"><a href="../../../../../posts/tags/transformers.html">transformers</a></span>
                                        <span class="tag-item"><a href="../../../../../posts/tags/attention.html">attention</a></span>
                                        <span class="tag-item"><a href="../../../../../posts/tags/nlp.html">NLP</a></span>
                                        <span class="tag-item"><a href="../../../../../posts/tags/machine_learning.html">machine learning</a></span>
                                        <span class="tag-item"><a href="../../../../../posts/tags/ai.html">AI</a></span>
                                    </div>    
                                </div>
                                </br>


                                <section>
                                <p>The Transformer architecture has ushered in a seismic shift in the ever-evolving world of Natural Language Processing (NLP). Imagine this: The titans of NLP, LLMs such as ChatGPT and BARD, have reshaped the way we engage with language. They excel in Natural Language Understanding (NLU), process unrivalled text completion skills, and astound with their text summarization prowess.</p>
                                <p>The Transformer architecture serves as the unifying thread that weaves through these extraordinary LLMs. It‚Äôs no longer merely a buzzword; it forms the bedrock of AI and Machine Learning. In this article, we‚Äôll demystify LLMs from a layman‚Äôs perspective and unravel the core principles that underpin Transformers.</p>
                                <p>Prepare to be captivated as we delve into the inner workings of Transformers and unveil the mathematical wizardry empowering them using a commonplace setting to reshape the NLP landscape. Welcome to the realm of the Transformers, where the future of language understanding and generation awaits your exploration. üöÄüìñ</p>

                                <h2>What is a Transformer?</h2>
                                <p>The Transformer architecture was first introduced by Vaswani et al, 2017 [1], a former senior staff research scientist at Google Brain team in their paper ‚ÄúAttention Is All You Need‚Äù which first introduced the concept of ‚Äúmulti-head attention‚Äù mechanism to the rest of the world. Jacob Uszkoreit coined the term ‚ÄúTransformers‚Äù.</p>
                                <p>Transformers is basically an architecture which uses various types of attention mechanisms (self-attention, multi-head attention, masked multi-head attention) replacing the traditional sequential models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) completely. They not only take lesser time to train, lesser carbon footprint and achieve state-of-the-art results on various benchmarks.</p>

                                <h3>What problems did Transformers solve?</h3>
                                <p>Transformers became a cornerstone in the field of AI/ML because it solved a couple of rather difficult problems with ease, namely:</p>
                                <ol>
                                <li>Mastering Long-Range Dependencies: Prior to Transformers, the AI world struggled with sequential models such as Long-Short Term Memory (LSTMs) and Gated Recurrent Units (GRUs) to handle complex sequences. These models performed well during training but struggled when faced with wide, complicated sequences. Transformers, on the other hand, breezed passed this hurdle thanks to their self-attention magic.</li>
                                <li>The Parallelization Revolution: By utilising parallel processing, transformers sparked a computational revolution. While classic sequential models struggled under the weight of compute complexity, Transformers welcomed parallelization with ease, substantially reducing processing times.</li>
                                <li>Massive Multitask Language Understanding (MMLU) Mastery: Transformers have won the title of MMLU champions. Traditional models were left in the dust by their natural capacity to interpret complicated language tasks in zero-shot or few-shot conditions. Tasks involving intricate language understanding, translation, and other complexities were handled with unrivalled dexterity.</li>
                                <li>Dominating NLP Arenas: Transformers have emerged as the indisputable champions of NLP. They beat their predecessors in every NLP domain imaginable, including text generation, language translation, content summarization, sentiment analysis, named entity recognition (NER), and multilingual comprehension.</li>
                                <li>Beyond Text ‚Äî Multimodal Prowess: Transformers broke free from the confines of text data. Their prowess extended into diverse realms, from crafting captivating image captions to deciphering spoken words with speech recognition. The world of multimodal behavior bowed before Transformers‚Äô versatility, paving the way for an array of groundbreaking applications.</li>
                                </ol>
                                <p>While this list merely scratches the surface of Transformers‚Äô conquests, it‚Äôs a testament to their sheer brilliance. Delve deeper, and you‚Äôll uncover an array of problems they‚Äôve elegantly resolved, laying the foundation for a new era in AI and machine learning.</p>
                                <p>Enough with the backdrop. Now, we‚Äôll uses an analogy to examine the Transformer design from this angle. So sit back, relax, and grab a cup of coffee as we explore the mysterious world of Transformers.</p>

                                <h2>Self-Attention Mechanism</h2>
                                <p>Self-attention is at the core of Transformers, and in order to fully understand Transformers, we need to first understand what exactly is attention. The authors [1] describe attention function as ‚Äúmapping a query and set of key-value pairs to an output‚Äù. We will look at this more in detail later but for now, let‚Äôs try to understand this with a simple example.</p>
                                <p>Imagine you‚Äôre in a classroom, and the teacher has just asked a fascinating question. Now, there are many eager students with their hands raised, each hoping to share their knowledge.</p>
                                    <div class="bigcenterimgcontainer">
                                    <img src="img/slf_attention_intuition.webp" alt style>
                                    </div>
                                    <div class="spaceafterimg">
                                    </div>    
                                <p>But here‚Äôs the cool part: the teacher‚Äôs attention isn‚Äôt fixed on one student. It‚Äôs like they have magical glasses that can adjust their focus. They look at each raised hand, but they pay more attention to the students who seem to know the answer best.</p>
                                <p>So, when a student confidently shoots their hand up and has a strong response, it‚Äôs as if their hand shines the brightest, and the teacher‚Äôs magical glasses zoom in on them. The teacher listens carefully, giving extra attention to that student‚Äôs explanation.</p>
                                <p>But the other students aren‚Äôt left out; their hands are still up, and the teacher can quickly switch their magical glasses to listen closely to someone else if they have something valuable to add.</p>
                                <p>In this way, just like in our classroom, self-attention helps words in a sentence decide which other words to pay more attention to. It‚Äôs like a teacher with magical focus, ensuring everyone gets heard and understood!</p>

                                <h2>Multi-Head Attention Mechanism</h2>
                                <p>In our magical classroom, not only does the teacher have those amazing adjustable focus glasses, but they also have multiple pairs of eyes. Yes, it‚Äôs like they have a bunch of eyes on their head, each with its own pair of glasses!</p>
                                <p>Now, when the teacher asks a question and students raise their hands, these different pairs of eyes can focus on different students simultaneously. It‚Äôs as if the teacher can pay attention to multiple answers at once.</p>
                                    <div class="bigcenterimgcontainer">
                                    <img src="img/multi_head_attention_intuition.webp" alt style>
                                    </div>
                                    <div class="spaceafterimg">
                                    </div>    
                                    <p>Imagine one pair of eyes zooms in on a student who‚Äôs great with facts and figures, while another pair looks at a student who‚Äôs fantastic at explaining things. Each pair of eyes listens closely to a different aspect of the students‚Äô answers.</p>
                                <p>Afterward, all these pairs of eyes come together, like a team of experts sharing what they‚Äôve seen and heard. The teacher then combines all this information to give a comprehensive response or feedback to the class.</p>
                                <p>So, in the world of language and Transformers, multi-head attention works just like our teacher‚Äôs multiple pairs of eyes. It helps the model focus on different parts of the input at the same time, making it super sharp at understanding various aspects of the text. It‚Äôs like having a classroom full of experts, each looking at different details and sharing their wisdom to solve complex language problems!</p>

                                <h2>Masked Multi-Head Attention Mechanism</h2>
                                <p>Now, picture our classroom, but this time, the teacher is giving a surprise quiz, and the students are answering by raising their hands. However, there‚Äôs a twist ‚Äî some of the students have a little paper mask on their hands!</p>
                                <p>These masked students have secret information they don‚Äôt want to reveal to the rest of the class. So, when they raise their hands, they only show their masks, hiding their actual answers.</p>
                                    <div class="bigcenterimgcontainer">
                                    <img src="img/masked_multi_head_attention_intuition.webp" alt style>
                                    </div>
                                    <div class="spaceafterimg">
                                    </div>    
                                <p>Now, the teacher, equipped with their multiple pairs of eyes and adjustable focus glasses, has a challenge. They need to pay attention to all the raised hands, but they can‚Äôt see what‚Äôs behind the masks of these secretive students.</p>
                                <p>So, the teacher‚Äôs eyes try to look past the masks, but they can‚Äôt get a clear view of what‚Äôs underneath. They still listen to these students, though, because they might share some information without revealing their secret.</p>
                                <p>In the world of Transformers, masked multi-head attention works similarly. It‚Äôs like some parts of the input data are hidden behind masks, and the model needs to pay attention to everything, even if it can‚Äôt see all the details. It‚Äôs especially useful for tasks where you need to predict or generate certain parts of the output while keeping other parts hidden, just like our masked students in the classroom quiz!</p>

                                <h2>Conclusion</h2>
                                <p>In the captivating world of Natural Language Processing (NLP), the emergence of Transformers has been nothing short of a revelation. These architectural wonders, inspired by the magic of a dynamic classroom, have not only solved longstanding challenges but have rewritten the rules of the AI game.</p>
                                <p>We embarked on a journey to understand the core of Transformers, much like eager students in that magical classroom. Self-attention, akin to students‚Äô active participation, showed us how words could dynamically adjust their focus, leading to profound language comprehension.</p>
                                <p>Multi-head attention, reminiscent of the teacher‚Äôs multiple pairs of eyes, allowed us to explore different aspects of language simultaneously. It was like having a classroom full of experts, each contributing their unique wisdom.</p>
                                <p>And then, we uncovered the intrigue of masked multi-head attention, akin to secretive students with hidden knowledge. Just as the teacher adapted to the challenge of obscured answers, Transformers excel at deciphering information, even when parts are concealed.</p>
                                <p>As we conclude our exploration, it‚Äôs evident that Transformers have transformed NLP. They‚Äôve brought us state-of-the-art language understanding, generation, and a world of applications that were once beyond reach.</p>
                                <p>The future is bound to witness even greater feats as Transformers continue to evolve, pushing the boundaries of what‚Äôs possible in the realm of AI and machine learning. The stage is set, and the magic of Transformers will continue to dazzle and inspire. So, as we bid adieu to this adventure, let‚Äôs remain ever curious and excited about what the world of Transformers holds next. üöÄü§ñüìö</p>



                                </section>

                                <div id="disqus_thread"></div>

                                <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
                                <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

                                <script src="../../../../../js/inlineDisqussions.js"></script>
                                <script src="../../../../../js/disqus.js"></script>
                            </div>
                        </div>
                        <div class="col-md-4" id="toc-wrapper">
                        </div>
                    </div>
                </div>
            </main>
        

            <footer id="footer">
                <div class="container">
                    Subscribe to the <a href="../../rss.xml">RSS feed</a>.
                    </br>
                    Built by <a href="https://github.com/oinkina">Oinkina</a> with
                    <a href="http://jaspervdj.be/hakyll">Hakyll</a> 
                    using <a href="http://getbootstrap.com/">Bootstrap</a>, 
                    <a href="http://www.mathjax.org/">MathJax</a>, and
                    <a href="http://disqus.com/">Disqus</a>.
                </div>
            </footer>
        </div>

    <!-- jQuery-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>

    <script src="../../bootstrap/js/bootstrap.min.js"></script>

    <script src="../../../../../highlight/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <script src="../../../../../js/footnotes.js"></script>
    <script src="../../../../../js/bootstrap-carousel.js"></script>
    <script src="../../../../../js/inlineDisqussions.js"></script>
    <script src="../../../../../js/toc.js"></script>


    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <noscript>Enable JavaScript for footnotes, Disqus comments, and other cool stuff.</noscript>

    </body>

</html>