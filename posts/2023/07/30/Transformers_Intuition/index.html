<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta content="width=device-width, initial-scale=1.0" name="viewport" />
    <title>Tech with Deb</title>
    <meta content="" name="description" />
    <meta content="" name="keywords" />

    <!-- Favicons -->
    <link href="../../../../../assets/img/favicon.png" rel="icon" />
    <link
      href="../../../../../assets/img/apple-touch-icon.png"
      rel="apple-touch-icon"
    />

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Raleway:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap"
      rel="stylesheet"
    />

    <!-- Vendor CSS Files -->
    <link
      href="../../../../../assets/vendor/bootstrap/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <link
      href="../../../../../assets/vendor/bootstrap-icons/bootstrap-icons.css"
      rel="stylesheet"
    />
    <link href="../../../../../assets/vendor/aos/aos.css" rel="stylesheet" />
    <link
      href="../../../../../assets/vendor/glightbox/css/glightbox.min.css"
      rel="stylesheet"
    />
    <link
      href="../../../../../assets/vendor/swiper/swiper-bundle.min.css"
      rel="stylesheet"
    />

    <!-- Main CSS File -->
    <link href="../../../../../assets/css/main.css" rel="stylesheet" />
    <!--Highlight-->
    <link
      href="../../../../../assets/highlight/styles/github.css"
      rel="stylesheet"
    />

    <script
      type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    ></script>
  </head>

  <body>
    <!-- Page Title -->
    <div class="page-title dark-background">
      <div
        class="container d-lg-flex justify-content-between align-items-center"
      >
        <h1 class="mb-2 mb-lg-0">Understanding Transformers</h1>
        <nav class="breadcrumbs">
          <ol>
            <li><a href="../../../../../index.html">Home</a></li>
            <li><a href="../../../../../blogs.html">Blogs</a></li>
          </ol>
        </nav>
      </div>
    </div>
    <!-- End Page Title -->

    <div id="wrap">
      <main id="content">
        <div class="container">
          <div class="row">
            <div class="col-md-12">
              <div id="body">
                <div class="info">
                  <p style="font-family: Roboto; font-size: 100%">
                    Posted:
                    <time datetime="2023-09-30">September 30, 2023</time> <br />
                    Last Updated:
                    <time datetime="2025-02-28">February 26, 2025</time>
                  </p>
                  <div class="tag-container">
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/neural_networks.html"
                        >neural networks</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/deep_learning.html"
                        >deep learning</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/transformers.html"
                        >transformers</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/attention.html"
                        >attention</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/nlp.html"
                        >NLP</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/machine_learning.html"
                        >machine learning</a
                      ></span
                    >
                    <span class="tag-item"
                      ><a href="../../../../../posts/tags/ai.html">AI</a></span
                    >
                  </div>
                </div>
                <section>
                  <p>
                    The Transformer architecture has ushered in a seismic shift
                    in the ever-evolving world of Natural Language Processing
                    (NLP). Imagine this: The titans of NLP, LLMs such as ChatGPT
                    and BARD (now Gemini), have reshaped the way we engage with
                    language. They excel in Natural Language Understanding
                    (NLU), process unrivalled text completion skills, and
                    astound with their text summarization prowess. In the heart
                    of the transformer architecture lies the attention mechanism
                    which has revolutionized the way we look at natural
                    language. Attention as the name suggests is basically a
                    communication mechanism which decides how much attention
                    needs to be given to which parts of the sentences (tokens).
                  </p>
                  <div class="bigcenterimgcontainer">
                    <img src="img/attention-mechanism.png" alt style />
                  </div>
                  <div class="spaceafterimg"></div>
                  <p>
                    Think of attention mechanism as the highlighter in the above
                    image which quickly finds out what the most important
                    sections of a given document. The Transformer architecture
                    serves as the unifying thread that weaves through these
                    extraordinary LLMs. It‚Äôs no longer merely a buzzword; it
                    forms the bedrock of AI and Machine Learning. In this
                    article, we‚Äôll demystify LLMs from a layman‚Äôs perspective
                    and unravel the core principles that underpin Transformers.
                  </p>
                  <p>
                    Prepare to be captivated as we delve into the inner workings
                    of Transformers and unveil the mathematical wizardry
                    empowering them using a commonplace setting to reshape the
                    NLP landscape. Welcome to the realm of the Transformers,
                    where the future of language understanding and generation
                    awaits your exploration. üöÄüìñ
                  </p>

                  <h2>What is a Transformer?</h2>
                  <p>
                    The Transformer architecture was first introduced by Vaswani
                    et al, 2017 [1], a former senior staff research scientist at
                    Google Brain team in their paper ‚ÄúAttention Is All You Need‚Äù
                    which first introduced the concept of ‚Äúmulti-head attention‚Äù
                    mechanism to the rest of the world. Jacob Uszkoreit coined
                    the term ‚ÄúTransformers‚Äù.
                  </p>
                  <div class="centerimgcontainer">
                    <img src="img/transformer_cover.jpg" alt style />
                  </div>
                  <div class="spaceafterimg"></div>
                  <p>
                    Transformers is basically an architecture which uses various
                    types of attention mechanisms (self-attention, multi-head
                    attention, masked multi-head attention) replacing the
                    traditional sequential models such as convolutional neural
                    networks (CNNs) and recurrent neural networks (RNNs)
                    completely. They not only take lesser time to train, lesser
                    carbon footprint and achieve state-of-the-art results on
                    various benchmarks.
                  </p>

                  <h3>What problems did Transformers solve?</h3>
                  <p>
                    Transformers became a cornerstone in the field of AI/ML
                    because it solved a couple of rather difficult problems with
                    ease, namely:
                  </p>
                  <ol>
                    <li>
                      Mastering Long-Range Dependencies: Prior to Transformers,
                      the AI world struggled with sequential models such as
                      Long-Short Term Memory (LSTMs) and Gated Recurrent Units
                      (GRUs) to handle complex sequences. These models performed
                      well during training but struggled when faced with wide,
                      complicated sequences. Transformers, on the other hand,
                      breezed passed this hurdle thanks to their self-attention
                      magic.
                    </li>
                    <li>
                      The Parallelization Revolution: By utilising parallel
                      processing, transformers sparked a computational
                      revolution. While classic sequential models struggled
                      under the weight of compute complexity, Transformers
                      welcomed parallelization with ease, substantially reducing
                      processing times.
                    </li>
                    <li>
                      Massive Multitask Language Understanding (MMLU) Mastery:
                      Transformers have won the title of MMLU champions.
                      Traditional models were left in the dust by their natural
                      capacity to interpret complicated language tasks in
                      zero-shot or few-shot conditions. Tasks involving
                      intricate language understanding, translation, and other
                      complexities were handled with unrivalled dexterity.
                    </li>
                    <li>
                      Dominating NLP Arenas: Transformers have emerged as the
                      indisputable champions of NLP. They beat their
                      predecessors in every NLP domain imaginable, including
                      text generation, language translation, content
                      summarization, sentiment analysis, named entity
                      recognition (NER), and multilingual comprehension.
                    </li>
                    <li>
                      Beyond Text ‚Äî Multimodal Prowess: Transformers broke free
                      from the confines of text data. Their prowess extended
                      into diverse realms, from crafting captivating image
                      captions to deciphering spoken words with speech
                      recognition. The world of multimodal behavior bowed before
                      Transformers‚Äô versatility, paving the way for an array of
                      groundbreaking applications.
                    </li>
                  </ol>
                  <p>
                    While this list merely scratches the surface of
                    Transformers‚Äô conquests, it‚Äôs a testament to their sheer
                    brilliance. Delve deeper, and you‚Äôll uncover an array of
                    problems they‚Äôve elegantly resolved, laying the foundation
                    for a new era in AI and machine learning.
                  </p>
                  <p>
                    Enough with the backdrop. Now, we‚Äôll uses an analogy to
                    examine the Transformer design from this angle. So sit back,
                    relax, and grab a cup of coffee as we explore the mysterious
                    world of Transformers.
                  </p>

                  <h2>Pre-requisites</h2>
                  <p>
                    Before diving head first into the world of Transformers, we
                    need to understand a few key concept - tokens, embeddings,
                    positional embeddings. So, what is a token? A token is
                    basically a numeric (vectorized) representation of a word or
                    sentence or text.
                  </p>
                  <p>
                    Unlike humans, computers cannot understand natural language.
                    So in order to make computers understand language and its
                    context, we transform the text into tokens. This process is
                    known as tokenization.
                  </p>
                  <p>
                    The tokens upto a fixed length (embedding dimension) are
                    combined to form a vector, called embeddings. There are many
                    ways to do this, but that is a topic for a different day.
                  </p>
                  <p>
                    The next important concept is how words are arranged in a
                    sentence. Consider the sentence
                    <q>She only told him the truth</q>. If we slightly alter the
                    position of the words as <q>She told him only the truth</q>.
                    While both sentences are similar, they mean different
                    things.
                  </p>
                  <p>
                    Hence, we can note that the position of words in a sentence
                    is very important, and this must be accurately captured. To
                    achieve this we calculate positional embeddings of the input
                    text, which basically capture the sequential information of
                    the text (the position at which a particular word appears in
                    the sentence).
                  </p>

                  <h2>Self-Attention Mechanism</h2>
                  <p>
                    Self-attention is at the core of Transformers, and in order
                    to fully understand Transformers, we need to first
                    understand what exactly is attention. The authors [1]
                    describe attention function as ‚Äúmapping a query and set of
                    key-value pairs to an output‚Äù. We will look at this more in
                    detail later but for now, let‚Äôs try to understand this with
                    a simple example.
                  </p>
                  <p>
                    Imagine you‚Äôre in a classroom, and the teacher has just
                    asked a fascinating question. Now, there are many eager
                    students with their hands raised, each hoping to share their
                    knowledge.
                  </p>
                  <div class="centerimgcontainer">
                    <img src="img/slf_attention_intuition.webp" alt style />
                  </div>
                  <div class="spaceafterimg"></div>
                  <p>
                    But here‚Äôs the cool part: the teacher‚Äôs attention isn‚Äôt
                    fixed on one student. It‚Äôs like they have magical glasses
                    that can adjust their focus. They look at each raised hand,
                    but they pay more attention to the students who seem to know
                    the answer best.
                  </p>
                  <p>
                    So, when a student confidently shoots their hand up and has
                    a strong response, it‚Äôs as if their hand shines the
                    brightest, and the teacher‚Äôs magical glasses zoom in on
                    them. The teacher listens carefully, giving extra attention
                    to that student‚Äôs explanation.
                  </p>
                  <p>
                    But the other students aren‚Äôt left out; their hands are
                    still up, and the teacher can quickly switch their magical
                    glasses to listen closely to someone else if they have
                    something valuable to add.
                  </p>
                  <p>
                    In this way, just like in our classroom, self-attention
                    helps words in a sentence decide which other words to pay
                    more attention to. It‚Äôs like a teacher with magical focus,
                    ensuring everyone gets heard and understood!
                  </p>

                  <h2>Multi-Head Attention Mechanism</h2>
                  <p>
                    In our magical classroom, not only does the teacher have
                    those amazing adjustable focus glasses, but they also have
                    multiple pairs of eyes. Yes, it‚Äôs like they have a bunch of
                    eyes on their head, each with its own pair of glasses!
                  </p>
                  <p>
                    Now, when the teacher asks a question and students raise
                    their hands, these different pairs of eyes can focus on
                    different students simultaneously. It‚Äôs as if the teacher
                    can pay attention to multiple answers at once.
                  </p>
                  <div class="centerimgcontainer">
                    <img
                      src="img/multi_head_attention_intuition.webp"
                      alt
                      style
                    />
                  </div>
                  <div class="spaceafterimg"></div>
                  <p>
                    Imagine one pair of eyes zooms in on a student who‚Äôs great
                    with facts and figures, while another pair looks at a
                    student who‚Äôs fantastic at explaining things. Each pair of
                    eyes listens closely to a different aspect of the students‚Äô
                    answers.
                  </p>
                  <p>
                    Afterward, all these pairs of eyes come together, like a
                    team of experts sharing what they‚Äôve seen and heard. The
                    teacher then combines all this information to give a
                    comprehensive response or feedback to the class.
                  </p>
                  <p>
                    So, in the world of language and Transformers, multi-head
                    attention works just like our teacher‚Äôs multiple pairs of
                    eyes. It helps the model focus on different parts of the
                    input at the same time, making it super sharp at
                    understanding various aspects of the text. It‚Äôs like having
                    a classroom full of experts, each looking at different
                    details and sharing their wisdom to solve complex language
                    problems!
                  </p>

                  <h2>Masked Multi-Head Attention Mechanism</h2>
                  <p>
                    Now, picture our classroom, but this time, the teacher is
                    giving a surprise quiz, and the students are answering by
                    raising their hands. However, there‚Äôs a twist ‚Äî some of the
                    students have a little paper mask on their hands!
                  </p>
                  <p>
                    These masked students have secret information they don‚Äôt
                    want to reveal to the rest of the class. So, when they raise
                    their hands, they only show their masks, hiding their actual
                    answers.
                  </p>
                  <div class="centerimgcontainer">
                    <img
                      src="img/masked_multi_head_attention_intuition.webp"
                      alt
                      style
                    />
                  </div>
                  <div class="spaceafterimg"></div>
                  <p>
                    Now, the teacher, equipped with their multiple pairs of eyes
                    and adjustable focus glasses, has a challenge. They need to
                    pay attention to all the raised hands, but they can‚Äôt see
                    what‚Äôs behind the masks of these secretive students.
                  </p>
                  <p>
                    So, the teacher‚Äôs eyes try to look past the masks, but they
                    can‚Äôt get a clear view of what‚Äôs underneath. They still
                    listen to these students, though, because they might share
                    some information without revealing their secret.
                  </p>
                  <p>
                    In the world of Transformers, masked multi-head attention
                    works similarly. It‚Äôs like some parts of the input data are
                    hidden behind masks, and the model needs to pay attention to
                    everything, even if it can‚Äôt see all the details. It‚Äôs
                    especially useful for tasks where you need to predict or
                    generate certain parts of the output while keeping other
                    parts hidden, just like our masked students in the classroom
                    quiz!
                  </p>

                  <h2>Conclusion</h2>
                  <p>
                    In the captivating world of Natural Language Processing
                    (NLP), the emergence of Transformers has been nothing short
                    of a revelation. These architectural wonders, inspired by
                    the magic of a dynamic classroom, have not only solved
                    longstanding challenges but have rewritten the rules of the
                    AI game.
                  </p>
                  <p>
                    We embarked on a journey to understand the core of
                    Transformers, much like eager students in that magical
                    classroom. Self-attention, akin to students‚Äô active
                    participation, showed us how words could dynamically adjust
                    their focus, leading to profound language comprehension.
                  </p>
                  <p>
                    Multi-head attention, reminiscent of the teacher‚Äôs multiple
                    pairs of eyes, allowed us to explore different aspects of
                    language simultaneously. It was like having a classroom full
                    of experts, each contributing their unique wisdom.
                  </p>
                  <p>
                    And then, we uncovered the intrigue of masked multi-head
                    attention, akin to secretive students with hidden knowledge.
                    Just as the teacher adapted to the challenge of obscured
                    answers, Transformers excel at deciphering information, even
                    when parts are concealed.
                  </p>
                  <p>
                    As we conclude our exploration, it‚Äôs evident that
                    Transformers have transformed NLP. They‚Äôve brought us
                    state-of-the-art language understanding, generation, and a
                    world of applications that were once beyond reach.
                  </p>
                  <p>
                    The future is bound to witness even greater feats as
                    Transformers continue to evolve, pushing the boundaries of
                    what‚Äôs possible in the realm of AI and machine learning. The
                    stage is set, and the magic of Transformers will continue to
                    dazzle and inspire. So, as we bid adieu to this adventure,
                    let‚Äôs remain ever curious and excited about what the world
                    of Transformers holds next. üöÄü§ñüìö
                  </p>
                </section>
                <section>
                  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
                  <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
                  <h2>Recommended Reads</h2>
                  <ul>
                    <li>
                      <a href="https://arxiv.org/abs/1706.03762">
                        Attention Is All You Need (Paper) - Vaswani et al. 2017
                      </a>
                    </li>
                    <li>
                      <a
                        href="https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html"
                      >
                        Self-Attention from Scratch - Sebastian Raschka
                      </a>
                    </li>
                    <li>
                      <a
                        href="https://youtu.be/fjJOgb-E41w?si=W816q5tHQTN1YXlh"
                      >
                        Attention Mechanism Explainer Video - Google Cloud
                      </a>
                    </li>
                    <li>
                      <a
                        href="https://youtu.be/PSs6nxngL6k?si=_qt8f_58DtdQsg3S"
                      >
                        Attention Clearly Explained Video - Joshua Starmer
                        (StatQuest)
                      </a>
                    </li>
                  </ul>
                </section>
              </div>
              <div class="col-md-4" id="toc-wrapper"></div>
            </div>
          </div>
        </div>
      </main>
    </div>

    <footer id="footer" class="footer position-relative light-background">
      <div class="container">
        <div class="copyright text-center">
          <p>
            ¬© <span>Copyright</span>
            <strong class="px-1 sitename">Debanjan Saha</strong>
            <span>All Rights Reserved</span>
          </p>
        </div>
        <div class="credits">
          <!-- All the links in the footer should remain intact. -->
          <!-- You can delete the links only if you've purchased the pro version. -->
          <!-- Licensing information: https://bootstrapmade.com/license/ -->
          <!-- Purchase the pro version with working PHP/AJAX contact form: [buy-url] -->
          Designed by
          <a href="https://bootstrapmade.com/">BootstrapMade</a> Distributed by
          <a href="https://themewagon.com">ThemeWagon</a>
        </div>
      </div>
    </footer>

    <!-- Scroll Top -->
    <a
      href="#"
      id="scroll-top"
      class="scroll-top d-flex align-items-center justify-content-center"
      ><i class="bi bi-arrow-up-short"></i
    ></a>

    <!-- jQuery-->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
    <script src="../../../../../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="../../../../../assets/js/main.js"></script>

    <script src="../../../../../assets/highlight/highlight.pack.js"></script>
    <script>
      hljs.initHighlightingOnLoad();
    </script>

    <script src="../../../../../assets/js/footnotes.js"></script>
    <script src="../../../../../assets/js/bootstrap-carousel.js"></script>
    <script src="../../../../../assets/js/inlineDisqussions.js"></script>
    <script src="../../../../../assets/js/toc.js"></script>

    <script
      type="text/javascript"
      src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    ></script>

    <noscript
      >Enable JavaScript for footnotes, Disqus comments, and other cool
      stuff.</noscript
    >
  </body>
</html>
